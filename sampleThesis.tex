\documentclass[phd,tocprelim]{cornell}
%% Added to avoid the \ifpdf name clash error
\let\ifpdf\relax
%
% tocprelim option must be included to put the roman numeral pages in the
% table of contents
%
% The cornellheadings option will make headings completely consistent with
% guidelines.
%
% This sample document was originally provided by Blake Jacquot, and
% fixed up by Andrew Myers.
%
%Some possible packages to include
\usepackage{graphicx,pstricks}
\usepackage{graphics}
\usepackage{moreverb}
\usepackage{subfigure}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{hangcaption}
\usepackage{txfonts}
\usepackage{palatino}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
%\usepackage{ruler}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath,amssymb} % define this before the line numbering.
%\usepackage{ruler}
\usepackage{color}
\usepackage{comment}
\usepackage{bm,nicefrac} % bold math package
\usepackage{amsmath}
\usepackage{tikz}
\DeclareMathOperator*{\argmax}{argmax}
\usepackage{hyperref}
\usepackage{color}
\usepackage{threeparttable}
\usepackage[linesnumbered, ruled]{algorithm2e}
\SetKwRepeat{Do}{do}{while}
\usepackage{graphicx}

%\usepackage{fancyhdr}
%\setlength{\headheight}{15pt}
%\pagestyle{fancy}
%\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
%\renewcommand{\sectionmark}[1]{\markright{#1}{}}

\usepackage{amsmath}
\DontPrintSemicolon
%\usepackage{algorithm}
%\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath,amssymb} % define this before the line numbering.
%\usepackage{ruler}
\usepackage{color}
\usepackage{amsmath}
%\DeclareMathOperator*{\argmax}{argmax}
\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = black, %Colour for external hyperlinks
  linkcolor    = red, %Colour of internal links
  %citecolor   = red %Colour of citations
  citecolor = blue %black 
}
\usepackage{lineno,hyperref}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
%\usepackage{ruler}
\usepackage{color}
\usepackage{threeparttable}
%\usepackage[linesnumbered, ruled]{algorithm2e}
\modulolinenumbers[4]
\usepackage[noend]{algpseudocode}
\usepackage{amsmath,amssymb} % define this before the line numbering.
%\usepackage{ruler}
\usepackage{color}
\usepackage{comment}
\usepackage{bm,nicefrac} % bold math package
\usepackage{amsmath}
\usepackage{tikz}
%\DeclareMathOperator*{\argmax}{argmax}
\usepackage{hyperref}


%if you're having problems with overfull boxes, you may need to increase
%the tolerance to 9999
\tolerance=9999

\bibliographystyle{plain}
%\bibliographystyle{IEEEbib}

\renewcommand{\caption}[1]{\singlespacing\hangcaption{#1}\normalspacing}
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}

\title {Enhance Text Spotting with Visual Context information}
\author {Ahmed Sabir}
\conferraldate {August}{2020}
\degreefield {Ph.D.}
\copyrightholder{Ahmed Sabir}
\copyrightyear{2020}

\begin{document}

\maketitle
\makecopyright

\begin{abstract}
Recognizing text   in  images in the wild is an unsolved problem  in computer vision. Many  approaches are based on purely visual information and ignore the semantic relation between scene and text. In this paper, we further exploit Natural Language Processing  ideas and propose a hybrid post-processing approach to improve scene recognition accuracy using the semantic correlation between  \textit{text in the wild} and its visual context. For this purpose,  we resort to semantic information extracted from textual corpus to improve state-of-the-art visual deep learning methods. Specifically, we initially rely on an off-the-shelf deep neural network (DNN) that provides a series of text hypotheses per input text image. These text hypotheses are then re-ranked using 1) word frequency, 2) relatedness to other objects in the image, 3) relatedness to image scenario, and 4) relatedness to a synthetic natural language description of the image (caption). As a result of this visual information, the performance of the original DNN is boosted with a very small training cost. We demonstrate the effectiveness of our approach on COCO-text and ICDAR'17 datasets. 

In my thesis I explored several techniques to improve how to efficiently model signal
representations and learn useful information from them.  The building block of my disserta-
tion is based on machine learning approaches to classification, where a (typically non-linear)
function is learned from labeled examples to map from signals to some useful information
(e.g.  an object class present an image, or a word present in an acoustic signal).  One of the
motivating factors of my work has been advances in neural networks in deep architectures
(which  has  led  to  the  terminology  “deep  learning”),  and  that  has  shown  state-of-the-art
performance in acoustic modeling and object recognition – the main focus of this thesis.
In  my  work,  I  have  contributed  to  both  the  learning  (or  training)  of  such  architectures
through  faster  and  robust  optimization  techniques,  and  also  to  the  simplification  of  the
deep architecture model to an approach that is simple to optimize.  Furthermore, I derived
a  theoretical  bound  showing  a  fundamental  limitation  of  shallow  architectures  based  on
sparse coding (which can be seen as a one hidden layer neural network), thus justifying the
need for deeper architectures, while also empirically verifying these architectural choices on
speech recognition.  Many of my contributions have been used in a wide variety of applica-
tions, products and datasets as a result of many collaborations within ICSI and Berkeley,
but also at Microsoft Research and Google Research.
graduate school.
\end{abstract}

%\begin{biosketch}
%Your biosketch goes here. Make sure it sits inside
%the brackets.
%\end{biosketch}

\begin{dedication}
This document is dedicated to the citizen of mars .
\end{dedication}

\begin{acknowledgements}
Your acknowledgements go here. Make sure it sits inside the brackets.
In my thesis I explored several techniques to improve how to efficiently model signal
representations and learn useful information from them.  The building block of my disserta-
tion is based on machine learning approaches to classification, where a (typically non-linear)
function is learned from labeled examples to map from signals to some useful information
(e.g.  an object class present an image, or a word present in an acoustic signal).  One of the
motivating factors of my work has been advances in neural networks in deep architectures
(which  has  led  to  the  terminology  “deep  learning”),  and  that  has  shown  state-of-the-art
performance in acoustic modeling and object recognition – the main focus of this thesis.
In  my  work,  I  have  contributed  to  both  the  learning  (or  training)  of  such  architectures
through  faster  and  robust  optimization  techniques,  and  also  to  the  simplification  of  the
deep architecture model to an approach that is simple to optimize.  Furthermore, I derived
a  theoretical  bound  showing  a  fundamental  limitation  of  shallow  architectures  based  on
sparse coding (which can be seen as a one hidden layer neural network), thus justifying the
need for deeper architectures, while also empirically verifying these architectural choices on
speech recognition.  Many of my contributions have been used in a wide variety of applica-
tions, products and datasets as a result of many collaborations within ICSI and Berkeley,
but also at Microsoft Research and Google Research.
\end{acknowledgements}

\contentspage
\tablelistpage
\figurelistpage

\normalspacing \setcounter{page}{1} \pagenumbering{arabic}
\pagestyle{cornell} \addtolength{\parskip}{0.5\baselineskip}

\chapter{Introduction}

Reading letters and words is very important task in today’s society. Written and printed texts are everywhere, in form of newspapers, documents, text in the wild, etc. Machine replication of human vision, like reading, has been a dream of scientists and researchers for long time. Over the last five decades, machine reading, computer vision application has grown from a dream to reality. However, research in some area such as \textit{text recognition} in the wild has not reached a mature enough level in implementation, there are still many challenges due to the many possible variations in textures, backgrounds, fonts, and lighting conditions that are present in such images. Scanned document recognition have grown rapidly on different application, paper digitalization and camera based application is everywhere. The success of Optical Character Recognition system (OCR) has a long history in computer vision. However, the success of OCR system is restricted to scanned documents. Scene text exhibits a large variability in appearances, and can prove to be challenging even for the state-of-the-art OCR methods. Many scene text understanding methods recognize objects and regions like roads, trees, sky in the image successfully, but tend to ignore the text on the sign boards. Our goal is to fill this gap in understanding the scene. In addition, the automatic detection and recognition of text in natural images, \textit{text spotting }is an important challenge for visual understanding. 
%Nowadays, the modern wold is designed to be interpreted though use of label texts and texts like images. Therefore, text spotting system allow machine interaction with human world.

%One of the area where 
There are several areas where text spotting and recognition system aids is needed, such as screen readers that can help blind users and those with low vision to access documents \cite{nazma2016camera}. There are few systems \cite{merler2007recognizing,rajkumar2014portable} that can provide reliable access to the text printed on common handhold items such as product packages and prescription medication bottles. Figure \ref{fig:Motivation} shows several example of texts in the wild.



%One of the area where 
There are several areas where text spotting and recognition system aids is needed, such as screen readers that can help blind users and those with low vision to access documents \cite{nazma2016camera}. There are few systems \cite{merler2007recognizing,rajkumar2014portable} that can provide reliable access to the text printed on common handhold items such as product packages and prescription medication bottles. Figure \ref{fig:Motivation} shows several example of texts in the wild. 

% \begin{figure}[h!]
% \centering
% %\includegraphics[scale=0.3\textwidth]{mo.pdf}
% \caption{Text is found in a variety of different formats and scenarios in images
% and videos.  The range of fonts, textures, orientations, noise, and background objects
% are some of the challenges associated with text spotting.  However, this is a useful
% problem to solve, as often it is the text contained which captures the semantics of
% an image.}
% \end{figure}

 \begin{figure} 
\centering    
\includegraphics[width=1.0\textwidth]{13.pdf}
\caption{Text is found in a variety of different formats and scenarios in images
and videos.  The range of fonts, textures, orientations, noise, and background objects
are some of the challenges associated with text spotting.  However, this is a useful
problem to solve, as often it is the text contained which captures the semantics of
an image.}
\label{fig:Motivation}
 \end{figure}




Another area where complex background text spotting and recognition might be of a great use, is in %is of great use in
autonomous driving \cite{wang2012end}. By being able to interpret signs in the street with text
spotting, the system may be able to understand and gather information, and be able to 
interact. For example, an autonomous car driving system may need to read signs
%to understand the rules of the road and assist the driver.  
to understand the rules of the road to assist the driver.

\smallskip
Another useful application of text spotting and recognition is image retrieval. In this case, the system would be able to search for specific text in large scale images search, text spotting can be used to retrieve specific text in an images in a big database. For example, the system can search of text in a video or TV recording library and retrieve specific material \cite{jaderberg2014deep}.  




%Machine reading has shown a remarkable progress in  Optical Character Recognition systems (OCR). However, the success of most OCR systems is restricted to simple-background and properly aligned documents, while text in many real images  is affected by a number of artifacts including  partial occlusion,  perspective distortion and complex backgrounds. OCR in the wild  is far more challenging than traditional OCR, where the state-of-the-art has already surpassed human-level performance in applications such as book digitization and book reading for visually impaired. In short, developing OCR systems able to read text in the wild (traffic signs, advertisements, brands in clothing, street names, etc.) is still an open problem. In the computer vision community, this problem is known  as \textit{Text Spotting}. 

%However, while state-of-the-art computer vision algorithms have shown remarkable results in recognizing object instances in unrestricted images, understanding and recognizing the  text in a robust manner is far from being considered a solved problem.  

%Text spotting pipelines usually split the problem in two phases: 1) a \textit{text detection stage}, to estimate the bounding box around the candidate word in the image and 2) a \textit{text recognition stage}, to identify the text inside the bounding boxes. In this paper we focus on the second stage, and introduce a simple but efficient  post-processing approach  that  will take the candidate hypotheses from the second stage and re-rank them using their semantic relatedness with the visual context in the image. This relatedness is computed using a variety of Natural Language Processing (NLP) techniques.


%\textit{Text spotting} (or end-to-end text recognition),  refers to the problem of automatically detecting and recognizing text in images in the wild. Text spotting may be tackled by either a  lexicon-based or a lexicon-free perspective. Lexicon-based recognition methods use a pre-defined dictionary as  a reference to guide the recognition. Lexicon-free methods (or unconstrained recognition techniques),   predict   character sequences without relying on any dictionary. Most recently deep learning approaches are dominating the state-of-the-art method in text recognition. For instance, PhotoOCR  \cite{Alessandro:13} uses a Deep Neural Network (DNN) that performs end-to-end text spotting using histograms of oriented gradients  as input of the network. It is a lexicon-free  system able to read characters in uncontrolled conditions. Although this approach relies on a huge amount of annotated character bounding boxes, the final word re-ranking is performed by means of two language models, namely a character and an $N$-gram language model. %They  combined two language models, a character based bi-gram model with compact 8-gram and 4-gram word-level model.    
%Another approach employed language model for final word re-ranking \cite{Anand:12}. The top-down integration can tolerate the error in text detection or mis-recognition. The author employ a bi-gram model to overcome the inter-character confusion detection. 

%\cite{Max:16} introduces another DNN based approach   which applies a sliding window over Convolutional Neural Network (CNN) features that use a fixed-lexicon based dictionary. The same work also proposes a two separate character CNN \cite{jaderberg2014deep} that rely on character language models. The first CNN uses character sequence  model, and the other uses an $n$-gram encoding statistical language model. In \cite{Baoguang:16} the problem is addressed using a Recurrent CNN, a novel lexicon-free neural network architecture  that integrates  Convolutional and Recurrent Neural Networks for image based sequence recognition. Similarly, \cite{Suman:17} proposes a sequence recognition approach  that uses LSTM with visual attention mechanism for character prediction. \textcolor{black}{
%Although this method is  lexicon-free, it includes a language model to improve the accuracy. 
%\cite{Yunze:17} introduced a CNN with connectionist temporal classification (CTC) to generate the final label sequence without a sequence  model such as LSTM. This approach uses stacked convolutional layers to capture the dependencies of the input sequence. Most recently, \cite{fang2018attention} introduced a CNN based  encoder-decoder architecture. The encoder extracts image features from the scene text and converts it to feature maps. The decoder generates character sequence. }

%\cite{Alex:06}
%This is further extended in \cite{Max:14}, through a deep architecture that allows feature sharing.

%Nevertheless, deep learning methods --either lexicon-based or lexicon-free-- have drawbacks: Lexicon-based approaches need a large dictionary to perform the final recognition. Thus, their accuracy will depend on the quality and coverage of this lexicon, which makes this approach unpractical for real world applications where the domain may be different to that the system was trained on. On the other hand, lexicon-free recognition methods rely on sequence models to predict character sequences, and thus they may generate likely sentences that do not correspond to actual language words. In both cases, these techniques rely on the availability of large datasets to train and validate, which may not be always available for the target domain.



\begin{figure}
\centering 

%\includegraphics[width=2.5in]{example-1.pdf} 
%\includegraphics[width=\textwidth]{intro.pdf} 
\includegraphics[width=\textwidth]{intro1.pdf} 
\caption{overviewdsdsddsddsd dsdsd d sdsddf is the caption of making something better than other rhan rth.}
%Language Model
%thanks to the frequency count dictionary
 %\label{fig:all-data-example}
% \label{table_1}
 \end{figure} 


\section{Motivation}

%Nevertheless, deep learning methods --either lexicon-based or lexicon-free-- have drawbacks: Lexicon-based approaches need a large dictionary to perform the final recognition. Thus, their accuracy will depend on the quality and coverage of this lexicon, which makes this approach unpractical for real world applications where the domain may be different to that the system was trained on. On the other hand, lexicon-free recognition methods rely on sequence models to predict character sequences, and thus they may generate likely sentences that do not correspond to actual language words. In both cases, these techniques rely on the availability of large datasets to train and validate, which may not be always available for the target domain.



This research proposal addresses the problem of text recognition in images with complex background. We would like to have a system be able to spot and recognize text in the wild, text with projective distortions, non flat object label and common hand-held items with complex background. For example, recognizing a signage on the street -- if a human can read it, our system should be able to read it. 
\smallskip

Extracting any sort of semantic data from digital image is not a trivial task. Requiring multiple levels of feature extraction and inference tool have proposed for range of computer vision problems, including text spotting and word recognition. In this work, we focus on the use of deep learning based approaches with integration of additional external knowledge, in order to predict and improve recognition results. 
\\ 
The main target of this work is to contribute new methods %to increase the recognition accuracy of text spotting in a complex background.





to improve the recognition accuracy of text spotting system by introducing additional knowledge, such as linguistic model, and visual context bias. The main objectives of this research are the following :                            
%a language model. 

%Our proposed system consists of four models 1) detection models, 
%2) recognition model, 3) context bias  and 4) linguistic model.  However, the principal objective of %this work is to improve text recognition by utilizing different language model approaches. Therefore, this work
%Our main goal is to contribute a new methods to improve test recognition by utilizing different external prior information. Therefore, this work


%this work is to improve text recognition by utilizing different external prior information. Therefore, this work
%focuses on integrating different language models with recognition stage, to present a system will set a 
%focuses on integration of extra knowledge, such as linguistic model, to present a system able to understand the %scene,and set a benchmark in text recognition. The main objectives of this research are the following :
%scene. 


- Validate the utility of introducing linguistic model into deep learning algorithms model (DLM). By integrating a linguistic model, our system will be able to recognize text. Deep learning networks have excellent results in many problem such as handwriting recognition and text spotting without linguistic model. However, recognition results show that the absence of linguistic model affects negatively the recognition rate. For example, DLM has challenges recognizing letter with accent in latin languages or symbols in a text. % or two digit number inside text   

-Validate the utility of introducing Visual Context Bias information (VCB) into the text recognition model. VCB methods such as semantic relatedness \cite{leong2011measuring} use relation between two terms or word to select the correct output. For example word “car” is similar to word “motorcycle”, but is also related to word “road” and word “driving”. An example of VCB, an object classifier (OC) with linguistic model. %the output will be integrated with recognition model. 
The OC will assist the system understand the environment, such as location. For instance, if the OC detected a word “tunnel” then the text occurring in the image are more likely to be semantically related to traffic or transportation domains. 


















\section{Structure of the Thesis}
Section 2 text.
%3


\section{Claims of the Thesis}

\section{Contributions}
This section, we list the main contribution made in this thesis
\begin{itemize}
    \item dsdsdsdfdfdf 
    \item dsdsdsdffdfd
\end{itemize}
\section{Publication}
This section gives a list of publication that contain the cotnext of this thesis
\begin{itemize}
    \item sdsdsdsd
\end{itemize}
%\subsection{Subsection heading goes here}


%\chapter{Overview of Scene Test Recognition }
\chapter{Literature Review}
\section{Test spotting}
\subsection{Text detection}
\subsection{Text recognition}
\section{Visual context for text spotting}
\section{Information retrieval with textual context}

Understanding the visual environment around the text is very import for scene understanding. \cite{zhu2016could} shows that the visual context could be beneficial for text detection. This work uses a 14 classes pixel classifier to extract \textit{context features} from image, such as \textit{tree, river, wall} to assist scene text detection. ~\cite{patel2016dynamic} uses visual prior information to improve text spotting task and Latent Dirichlet Allocation (LDA) \cite{blei2003latent} to generate a new lexicon. The topic modeling learns the relation between text and images. \textcolor{black}{The same work extended this approach with dual LDAs 1) from the entire text from Wikipedia that related to image and 2) from image caption. Then, both LDAs are concatenated with a visual feature from pre-traind object classifier \cite{patel2019self}. Another approach employs topic modeling to learn the correlation between visual context and the spotted text in social media \cite{kang2017detection}. The metadata associated with each image (e.g tags, comments and titles) is used as context to enhance recognition accuracy. \cite{karaoglu2017text} takes advantage of text and visual context for logo retrieval problem. This approach  however uses textual cues to retrieve logos based on the relation between text that appear in the logo and the logo itself. \cite{GomezMaflaECCV2018single} proposed  a scene text retrieval that uses the text in the scene to retrieve all related image with nearest neighbor search. This approach utilize PHOC (Pyramidal Histogram Of Characters) as a word representation to overcome the limitation of word-out-dictionary. Most recently, \cite{prasad_2018_ECCV} use object information that surround the spotted text, 42 text related object classes, to guide text detection. The author proposed two sub-networks and three stages training procedure to lean the relation between text and object class. For instance, the dependence between $car$-$plate$, $building$-$text$ and $sign board$-$digit$.}


 \textcolor{black}{Understanding the  \textit{visual} context around the text is very important for scene understanding problem.   \cite{zhu2016could} uses a 14-class pixel classifier to extract \textit{context features} from image background to assist scene text detection. 
 For example, if classifier output is \textit{tree, river} the probability of no-text is very high. Meanwhile, if the background is \textit{signboard} the classifier will try locate and detect the text. 
 The work \cite{karaoglu2017text} also takes advantage of text and visual context for logo retrieval problem. However, this approach uses textual cues to retrieve logos based on the relation between text that appears in the logo and the logo itself. 
 For example, finding the character trigrams \textit{sta, tar} raises the probability of \textit{starbucks} logo higher than any other.
 } 
 
 \textcolor{black}{Another work that  uses visual prior information to improve the text spotting task is \cite{Yash:16}. This work uses Latent Dirichlet Allocation (LDA) \cite{David:03} to generate a new lexicon. The topic modeling learns the relation between text and images. This approach, however, relies on captions describing the images rather than using the main key words semantically related to the images to generate the lexicon re-ranking. Thus, the lexicon generation can be inaccurate in some cases due to the short length of captions. The same work was  extended to dual LDAs: 1) from the entire text from Wikipedia that related to image, and 2) from image caption. Both LDAs are then concatenated with a visual feature from a pre-trained object classifier \cite{patel2019self}. Another approach employs topic modeling to learn the correlation between visual context and the spotted text in social media \cite{kang2017detection}. The metadata associated with each image (e.g tags, comments and titles) is used as context to enhance the recognition accuracy. \cite{GomezMaflaECCV2018single} proposed  a scene text retrieval that uses the text in the scene to retrieve all related image with nearest neighbor search. This approach utilizes PHOC (Pyramidal Histogram Of Characters) as a word representation to overcome the limitation of out-of-dictionary words. Most recently, \cite{prasad_2018_ECCV} uses information about objects surrounding the spotted text (42 text-related object classes) to guide text detection. The same work proposed two sub-networks and a three stages training procedure to lean the relation between text and object class. For instance, the dependence between $car$-$plate$, $building$-$text$ and $sign board$-$digit$.}



\section{Words Association} %relation
\subsection{Word embedding}

Human vocabulary comes in free text. In order to make a machine learning model understand and process the natural language, we need to transform the free-text words into numeric values. One of the simplest transformation approaches is to do a one-hot encoding in which each distinct word stands for one dimension of the resulting vector and a binary value indicates whether the word presents (1) or not (0).

However, one-hot encoding is impractical computationally when dealing with the entire vocabulary, as the representation demands hundreds of thousands of dimensions. Word embedding represents words and phrases in vectors of (non-binary) numeric values with much lower and thus denser dimensions. An intuitive assumption for good word embedding is that they can approximate the similarity between words (i.e., “cat” and “kitten” are similar words, and thus they are expected to be close in the reduced vector space) or disclose hidden semantic relationships (i.e., the relationship between “cat” and “kitten” is an analogy to the one between “dog” and “puppy”). Contextual information is super useful for learning word meaning and relationship, as similar words may appear in the similar context often.

There are two main approaches for learning word embedding, both relying on the contextual knowledge:

-Count-based: The first one is unsupervised, based on matrix factorization of a global word co-occurrence matrix. Raw co-occurrence counts do not work well, so we want to do smart things on top.
-Context-based: The second approach is supervised. Given a local context, we want to design a model to predict the target words and in the meantime, this model learns the efficient word embedding representation.

\begin{figure}
\centering 
%\includegraphics[width=3.5in]{word2vec-skip-gram.png}
\includegraphics[width=\textwidth]{skip-gram.pdf} 
\caption{The skip-gram model. Both the input vector x and the output y are one-hot encoded word representations. The hidden layer is the word embedding of size N}
\end{figure}



\subsubsection{Skip-gram}
Suppose that you have a sliding window of a fixed size moving along a sentence: the word in the middle is the “target” and those on its left and right within the sliding window are the context words. The skip-gram model \cite{} is trained to predict the probabilities of a word being a context word for the given target.

Given the vocabulary size \textit{}, we are about to learn word embedding vector of size \textit{N}. The model learn to predict one context word(output) using one target word(input) at a time. According to the figure:

\begin{itemize}\small 
\item Both input word $w_i$ and the output $w_j$ are one-hot encoded into binary vector \textbf{x} and \textbf{y} and size \textit{V}. 
\item  First, the multiplication of the binary vector \textbf{x}  and the word embedding \textit{W} of size $\textit{V} \times \textit{N}$ gives us the embedding vector of the input word $w_j$: the i-the row of the matrix \textit{W}. 
\item This newly discovered embedding vector of dimension \textit{N} forms the hidden layer.
\item The multiplication pf the hidden layer and the word context matrix W' of size  $\textit{N} \times \textit{W}$ produces the output one-hot encoded vector \textbf{y}.
\item the output context matrix W' encodes the meanings of words as context, different from the embedding matrix \textit{W}\footnote{\small \small note: despite the name, W′ is independent of W, not a transpose or inverse }. 
\end{itemize}



\begin{figure}
\centering 
%\includegraphics[width=3.5in]{word2vec-skip-gram.png}
\includegraphics[width=\textwidth]{man-women.png} 
\caption{Visualization of the famous example (\textit{king - man + woman is queen}) example that show how \textit{queen} is in the same semantic space of (\textit{King-man+woman}) }
\end{figure}




%The following example demonstrates multiple pairs of target and context words as training samples, generated by a 5-word window sliding along the sentence.


\subsubsection{Loss function}
\subsubsection{Softmax}
\subsubsection{Hierarchical softmax}
\subsubsection{Negative sampling}

The Negative sampling proposed by Mikolov 2013 is a simplified variation of NCE loss. Different from NCE loss which attempts to approximately maximize the log probability of the softmax output, \textbf{negative sampling} dud further simplification because it focuses on learning high-quality word embedding rather than modeling the word distribution in natural language.  NEG approximate the binary classifier's output with sigmoid function as follows:

\begin{align*}
p(d=1 \vert w_, w_I) &= \sigma({v'_{w}}^\top v_{w_I}) \\
p(d=0 \vert w, w_I) &= 1 - \sigma({v'_{w}}^\top v_{w_I}) = \sigma(-{v'_{w}}^\top v_{w_I})
\end{align*} %]]>

The final NCE loss function looks like:
\begin{equation}
\mathcal{L}_\theta = - [ \log \sigma({v'_{w}}^\top v_{w_I}) +  \sum_{\substack{i=1 \\ \tilde{w}_i \sim Q}}^N \log \sigma(-{v'_{\tilde{w}_i}}^\top v_{w_I})]
\end{equation}






\subsubsection{Noise Contrastive Estimation}

\subsection{Similarity Function}
\subsubsection{Cosine Similarity}




why cosine ? 
Cosine similarity is generally used as a metric for measuring distance when the magnitude of the vectors does not matter. This happens for example when working with text data represented by word counts. We could assume that when a word (e.g. science) occurs more frequent in document 1 than it does in document 2, that document 1 is more related to the topic of science. However, it could also be the case that we are working with documents of uneven lengths (Wikipedia articles for example). Then, science probably occurred more in document 1 just because it was way longer than document 2. Cosine similarity corrects for this.

Text data is the most typical example for when to use this metric. However, you might also want to apply cosine similarity for other cases where some properties of the instances make so that the weights might be larger without meaning anything different. Sensor values that were captured in various lengths (in time) between instances could be such an example.







\subsection{Deep learning for Text}
\subsubsection{1D Convolutional neural network}
\subsubsection{Long Short Term Memory}
\subsubsubsection{Attention}

Understanding the  \textit{visual} context around the text is very important for scene understanding problem.   \cite{zhu2016could} uses a 14-class pixel classifier to extract \textit{context features} from image background to assist scene text detection. 
 For example, if classifier output is \textit{tree, river} the probability of no-text is very high. Meanwhile, if the background is \textit{signboard} the classifier will try locate and detect the text. 
 The work \cite{karaoglu2017text} also takes advantage of text and visual context for logo retrieval problem. However, this approach uses textual cues to retrieve logos based on the relation between text that appears in the logo and the logo itself. 
 For example, finding the character trigrams \textit{sta, tar} raises the probability of \textit{starbucks} logo higher than any other.
 
 
 \textcolor{black}{Another work that  uses visual prior information to improve the text spotting task is \cite{Yash:16}. This work uses Latent Dirichlet Allocation (LDA) \cite{David:03} to generate a new lexicon. The topic modeling learns the relation between text and images. This approach, however, relies on captions describing the images rather than using the main key words semantically related to the images to generate the lexicon re-ranking. Thus, the lexicon generation can be inaccurate in some cases due to the short length of captions. The same work was  extended to dual LDAs: 1) from the entire text from Wikipedia that related to image, and 2) from image caption. Both LDAs are then concatenated with a visual feature from a pre-trained object classifier \cite{patel2019self}. Another approach employs topic modeling to learn the correlation between visual context and the spotted text in social media \cite{kang2017detection}. The metadata associated with each image (e.g tags, comments and titles) is used as context to enhance the recognition accuracy. \cite{GomezMaflaECCV2018single} proposed  a scene text retrieval that uses the text in the scene to retrieve all related image with nearest neighbor search. This approach utilizes PHOC (Pyramidal Histogram Of Characters) as a word representation to overcome the limitation of out-of-dictionary words. Most recently, \cite{prasad_2018_ECCV} uses information about objects surrounding the spotted text (42 text-related object) to guide text detection. The same work proposed two sub-networks and a three stages training procedure to lean the relation between text and object class. For instance, the dependence between $car$-$plate$, $building$-$text$ and $sign board$-$digit$.}

\section{\textcolor{red}{Challenges}}


%\subsubsection{Subsubsection 1 heading goes here}
Subsubsection 1 text

%\subsubsection{Subsubsection 2 heading goes here}
%Subsubsection 2 text

\section{SECTION 3}
Section 3 text. The dielectric constant at the air-metal interface
determines the resonance shift as absorption or capture occurs.

\chapter{Data}
In this chapter we describe the datasets used in this thesis. Since the focus of this thesis is solely of adding visual context information to text spotting system, we used out-of-the-box tools to extract this textual information, ans so we introduce and describe them here. 

There are many pre-existing publicly available dataset for text spotting or end-to-end text recognition.  However, none of these datasets have textual information. Thus, we also  make our own contributions to the data in this field.

\begin{table}[h]

\centering
\small 
\caption{A verious End-to-End (Test spotting) dataset. The task is to localize and recognize the text in scene.}
\begin{tabular}{|c|l|c|c|c|}
\hline 
\multicolumn{4}{|c|}{End-to-End Text Recognition Dataset}   \\
\hline \hline 
 Label &  Description  & Dictionary & \# images    \\
 \hline  \hline 
IC03 & ICDAR 2003  test dataset  \cite{lucas2003icdar} & -  & 251   \\
\hline 
IC03-50 & ICDAR 2003  test dataset  \cite{lucas2003icdar} & 50  & 251   \\
\hline 
IC03-Full & ICDAR 2003  test dataset  \cite{lucas2003icdar} & 860 & 251   \\
\hline 
SVT & Street view test dataset \cite{wang2010word}   & - & 249  \\
\hline 
SVT-50 & Street view dataset with fixed lexicon \cite{wang2010word}  & 50 & 249  \\
\hline 
IC17 & ICDAR 2017 \cite{veit2016coco} (COCO-text)  & - & 145K \\ 
\hline 
\end{tabular}
\label{tb:end-to-end-text spotting}
\end{table}


\begin{table}
%
\caption{A various word recognition dataset. The images are cropped word images for recognizing task only.}
\centering
%\small 
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|l|c|c|c|}

\hline  
\multicolumn{4}{|c|}{ Text Recognition Dataset}   \\
\hline \hline 
Label     &  Description  & Dictionary & \# images   \\

\hline    \hline 
\small  IC17&  \small  from COCO-text dataset \cite{veit2016coco}  for text recognition              &  \small  -  & \small  10k \\ 
\hline 
%\small  COCO-Text &  \small  Our extracted dataset with full image from COCO-text \cite{veit2016coco}                         &  \small  -  & \small  11k \\  %without texting
\small  COCO-Text &  \small  Our extracted dataset with full image from COCO-text \cite{veit2016coco}                         &  \small  -  & \small  16k \\ 
\hline 
\small Synth90k   &  \small Synthetic dataset with 90k dict \cite{jaderberg2014synthetic} (testing dataset)  & \small  90k & \small  9M   \\
\hline 
\small SynthRand  & \small Synthetic dataset with random text \cite{jaderberg2014synthetic}  (testing dataset)   & \small - & \small 9M  \\  
\hline 
SVT        &  \small  Street view \cite{wang2010word}  testing dataset & \small - & \small 647 \\ 
\hline 
IC03       & \small  ICDAR 2013 \cite{lucas2003icdar} testing dataset        &     -   &   \small  860 \\ 
\hline 
\end{tabular}

}
\label{tb:tex-recogition}
\end{table}


\begin{figure}
\centering 

%\includegraphics[width=2.5in]{example-1.pdf} 
\includegraphics[width=4.7in]{all-dataset-example.pdf} 
\caption{Some text spotting data sample from (a) COCO-text, (b) Google street view (SVT) showing the bounding box in green (annotation). (c) Some example from ICDAR03 recognition test dataset}
%Language Model
%thanks to the frequency count dictionary
 %\label{fig:all-data-example}
% \label{table_1}
 \end{figure} 



 %43,686 full images with 145,859 text instances.

\newpage
\section{Public Datasets}
In this section we describes a various publicly available text spotting datasset. 

\subsection{Synthetic Dataset}
%The two biggest synthetic datasets for training deep learning model was introduced by the same author. \cite{jaderberg2016reading, jaderberg2015deep}
As Shown in Table \ref{tb:tex-recogition}  the word images sample is only in thousands with a very limited vocabulary. Thus, \cite{jaderberg2014synthetic} introduced a synthetic data generator without any human label cost. Not to mention, all current State-of-the-art are trained on this datasets \cite{ghosh2017visual,shi2016end,jaderberg2016reading,jaderberg2015deep, shi2017end,gao2017reading, fang2018attention}


\subsubsection{Synth90k  \cite{jaderberg2014synthetic} }
This dataset is synthetically generated dataset that consists of 9 million word images with 90k word dictionary. The 90K dictionary consists of an English from an open source spell checking system call Hunspell.  The dictionary consists of 50K root words with prefixes and suffixes. Also, the author added words from other datasets (ICDAR, SVT, and IIIT). 

\subsubsection{SynthRand  \cite{jaderberg2014synthetic}}
The same author introduce another synthetic dataset of word images. The dataset consists of 8 million training images and a test set of 900K images. Unlike syth90k that use pre-defined dictionary, the words generated are completely random string of characters, where the maximum length of the generated characters is ten.

\begin{figure}
\centering 

%\includegraphics[width=2.5in]{example-1.pdf} 
\includegraphics[width=\textwidth]{Synthtic-data.png} 
\caption{Some random example from the synthetic dataset \cite{jaderberg2014synthetic}. The only big dataset is publicly available for training Deep learning model.}
%Language Model
%thanks to the frequency count dictionary
 %\label{fig:all-data-example}
% \label{table_1}
 \end{figure} 




\subsection{ICDAR Dataset}

\subsubsection{ICDAR 2013 \cite{lucas2003icdar}}
ICDAR 2013 datasets consists of two datasets 1) text localization and 2) text segmentation. Text localization consists of 328 training images and 233 test images. Figure \ref{fig:icdar} show some text spotting data examples from ICDAR 2013 dataset. ICDAR 2013 datasets represents for the evaluation of scene text understanding tasks: localization, segmentation, and recognition.

\subsubsection{ICDAR 2015}
images and 500 testing images. All images were taken by a camera, in variety of indoor/outdoor scenarios in multilingual environment and containing resolution images average size (1280*720) pixels.



\subsubsection{ICDAR 2017  \cite{veit2016coco} }
 %Figure \ref{fig:coco} show some text spotting data examples from COCO-text.
 
 
 The \textit{ICDAR-2017 Task3} aims for end-to-end text spotting (i.e. both detection and recognition). Thus, this dataset includes whole images, and the texts in them may appear rotated, distorted, or partially occluded. Since we focus only on text recognition, we use the ground truth detection as a golden detector to extract the bounding boxes from the full image. The dataset consists of 43,686 full images with 145,859 text instances, and for training 10,000 images and 27,550 instances for validation. We evaluate our approach on a subset of the validation containing 10,000 images with associated bounding boxes.
 


%\subsection{KAIST datasets \cite{lee2010scene} }
%KAIST datasets consists of 3000 images of indoor and outdoor scenes containing text, with both a mixture of photos from a high resolution (1600*1200) digital camera and a lower resolution (640*480) cell phone camera.
%Word and character bounding boxes are provided as well as segmentation maps of
%characters, and the words are a mixture of English and Korean. However, not all
%instances of text are labelled.

%KAIST datasets provides the accuracy of the training set ground truth annotations for the tasks of
%text localization and segmentation. For localization the ground truth is defined at the levels of words and individual %characters, and the evaluation framework is the same as for the ICDAR scene text dataset.


\subsection{Street View Text dataset }

This dataset contains of 349 images downloaded form google streetView \cite{wang2010word}. For each image, only one-word level bounding box is provided. This is the first dataset that deals with text image in a real scenario, in the wild, such as shop signs with a wide range of font and graph style. Also, 50-word lexicons are provided (SVT-50). 

%This  dataset  contains647 cropped word images downloaded from Google StreetView. In addition to results on totally unconstrained recog-nition we also report results using the predefined lexiconsdefined by







%\subsection{IIIT Scene Text Retrieval}
\subsection{COCO-Text }
%\cite{veit2016coco}
This datasets is based on the Microsoft COCO \cite{lin2014microsoft} (Common Objects in Context), its consists of 63,686 images, 173,589 text instance (annotations of the images). The COCO-Text datasets differs from the others datasets in three aspects. First, The datasets was not collected with text recognition in mind. Thus, the annotated text instances lie in their natural context. Second, its contains a wide of variety of text instances, such as machine printed and handwritten text. Finally, the COCO-Text has much larger scale than other datasets for text detection and recognition.
This is a challenge dataset with a lot of noise, false positive and illegible text since much of  text in the images is extracted by machine. 



%This section we evaluate the performance of the proposed approaches in the 
%{\bf ICDAR-2017-Task3 (end-to-end) } \cite{Andreas:16} dataset.

%This dataset is based on Microsoft COCO \cite{Tsung-Yi:14} (Common Objects in Context), which consists of 63,686 images, and 173,589 text instances (annotations of the images). COCO-Text was not collected with text recognition in mind, therefore, not all images contain textual annotations. %








\section{Textual datasets}
%Since there is no textual dataset 
While there are some publicly available datasets for text spotting, there is no textual dataset for this is task such as scene information and image description. We propose to introduce a textual dataset for the same task.  This section describes our textual dataset for text spotting.   


\subsection{General text corpora}


The probabilities of the unigram model are computed from a the {\textit{Opensubtitles}}\footnote{\href{https://opensubtitles.org}{https://opensubtitles.org}} \cite{lison2016opensubtitles2016} and {\textit{Google book n-gram}}\footnote{\href{https://books.google.com/ngrams}{ https://books.google.com/ngrams}} 
 text corpora (7 Million token). The main goal of ULM is to increase the probability of the most common words proposed by the baseline. 


\begin{table}[h]

\centering
\small 
\caption{Text happen in both image and text}
\begin{tabular}{c|l|c|c|c|c}
\hline 
\multicolumn{6}{|c|}{word-image-same-text}   \\
\hline \hline 
 word & \#   & word & \#  &  word &  \# \\
\hline \hline 
bus       & 7902     & stop          & 2345    & street     &  13849 \\  %1
airliner  & 4252     & parking       & 1928    & baseball   &  545  \\  %2
passenger & 3176     & electric      & 2085    &  \&        & 504 \\  %3
banana    & 1147     & restaurant    & 1213    &  hotdog    & 872 \\  %4
police    & 874      & pizza         & 1397    & analog     & 1859\\  %5
new       & 208      & mailbox       & 692     &   park     & 2663 \\  %6
motor     & 1561     & minibus       &  2066   & train      & 14  \\ %7
pay       & 202      &  moving       &  1079   &  911       &  45 \\ %9
trailer   & 828      &  cab          & 1312    &    desk    & 1226  \\ %10
ball      & 9744     & tennis        &   680   & traffic    & 4948   \\ %11
plate     & 1061     &  10           &   2060       &   12          &  1799   \\ %12

%p \$     &  parking & 4  & \&  & ping-pong &  16 \\ 
%p \$     &  parking & 4  & \&  & ping-pong &  16 \\ 
%SVT &           &        &  -    &    - \\ 

\hline 


\hline 
\end{tabular}
\label{tb:text-both-side}
\end{table}















%The language model is based on unigram, which based on language model N-gram.
%\begin{equation}

%P(w_1w_2...w_n)\;\approx\prod_iP(w_i)
%\end{equation} 
%\smallskip
%\subsubsection*{N-gram}
%The formalization of the idea of word prediction with probabilistic models called N-gram, which predict the next word form the previous n-1 words. Computing the probability of the next word will turn out to be closely related to computing sequence of words. N-grams are essential in any task in which we have to identify words in noisy and ambiguous input. For instance, in speech recognition, the input speech are noisy and most of the word extremely similar. In \cite{shilman2004spatial} give an intuition form handwriting recognition for how probability of word sequence can help. In spelling correction, finding the correct spelling error like \cite{kukich1992techniques} that introduced techniques for automatically correcting words in text. Beside, these area N-gram has also an application in part-of-speech tagging, natural language generation, word similarity and as well text prediction system in different systems like smart-phone. The use of N-gram of many applications, such as \cite{shilman2004spatial,kukich1992techniques} is the key inspiration for this language model in this work.  

%N-grams are used for a variety of different task. For example, when developing a language model, n-grams are used to develop not just unigram models but also bigram and trigram models. N-gram models that can be used in a variety of tasks such as spelling correction, word breaking and text summarization. In this work, we utilized one of many approaches of N-gram such as unigram based dictionary. The probabilities of the unigram model are computed from a the {\textit{Opensubtitles}}\footnote{\href{https://opensubtitles.org}{https://opensubtitles.org}} \cite{lison2016opensubtitles2016} and {\textit{Google book n-gram}}\footnote{\href{https://books.google.com/ngrams}{ https://books.google.com/ngrams}} 
% text corpora (7 Million token). The main goal of ULM is to increase the probability of the most common words proposed by the baseline. 
%%\begin{equation}
%\small P_{ULM}(w)\;=\;\frac{count(w_j)}{\sum_{w\in C} count(w)} 
%\end{equation}         



\begin{figure}
\centering 

%\includegraphics[width=2.5in]{example-1.pdf} 
\includegraphics[width=\textwidth]{COCO-freq-1.pdf} 
\caption{Frequency that objects in MS COCO co-occur with text \cite{veit2016coco} as can be seen that the presence of certain objects is very informative regarding text presence. Especially traffic and sports scenes almost always contain text and nature scenes with animals rarely contain contain text.}
%Language Model
%thanks to the frequency count dictionary
 \label{fig:coco-freq-figure}
% \label{table_1}
 \end{figure} 




%\section{Visual Context Information }
\subsection{Object information}
To capture the \textit{visual context} from each image, we will use out-of-the-box state-of-the-art visual object classifiers to extract the image context information that will be used to re-rank candidate words according to their semantic relatedness with the context.

We considered three pre-trained CNN classifiers: ResNet \cite{he2016deep}, GoogLeNet \cite{szegedy2015going} and Inception-Resnet-v2 \cite{szegedy2017inception}. The output of these classifiers is a $1000$-dimensional vector with the probabilities of $1000$ object instances. In this work we consider a threshold of most likely objects of the context predicted by the classifier. Although we extract the top-5 accuracy \textit{multiple visuals context} from each image, we use a threshold to filter out the probability predictions when the object classifier is not confident enough.
\subsection{Scene information}
Additionally, we considered a scene classifier \cite{zhou2017places} to extract scene information from each image. We used a pre-trained scene classifier {\textit{Places365-ResNet}}\footnote{\href{http://places2.csail.mit.edu/}{http://places2.csail.mit.edu/}} to extract scene categories. According to the authors of \textit{Places365-ResNet} the network achieved good result in \textit{top-5 accuracy}, which make it ideal for multiple visual context extraction. The output from this classifier is a $365$ scene categories. Also, we consider a threshold to extract most likely classes in the images, and eliminate low confidence predictions.  

\subsubsection{Object and text overlap}
This textual dataset have no bounding box but only the text in the image and visual happen together in the same image.  Also, this datasets is based on COCO-text, its consists of 158k pairs.

\begin{table}[h]

\centering
\small 
\caption{ dataset. }
\begin{tabular}{c|l|c|c|c|c}
\hline 
\multicolumn{6}{|c|}{Pair text-visual Overlapping Textual Datast}   \\
\hline \hline 
 Word &  Visual  & \#  &  Word  &  Visual &  \# \\
\hline \hline 
st      & street     & 527   &  way    &  street  & 316  \\
\hline 
stop    & street     & 1060  &  ave & street & 119          \\ 
\hline 
kia     & tennis     & 233       &  kt   &  racket  & 108    \\
\hline
delta   &  airliner  & 169    &  pizza   & dining &   44      \\ 
\hline 
airways & airliner   & 83    &  rain & umbrella & 16           \\ 
\hline 
w       & racket     & 149   & dell & desktop & 22              \\ 
\hline 
wii     & remote     & 35  &  puzzles & toyshop & 5              \\ 
\hline 
tennis  & racket     &72 &  p & parking & 146                     \\ 
\hline 
heineken & refrigerator & 11& bus & passenger & 39                 \\  
\hline 
cafe &  bakery & 5  & apple & grocery & 27                      \\ 
\hline 
%p \$     &  parking & 4  & \&  & ping-pong &  16 \\ 
%p \$     &  parking & 4  & \&  & ping-pong &  16 \\ 
%SVT &           &        &  -    &    - \\ 
25 & baseball & 14 & a  & volleyball & 45  \\ 
\hline 
starbucks & laptop & 100 & p \$     &  parking & 4 \\ 

\hline 
\end{tabular}
\label{tb:pair-overlaping-dataset}
\end{table}










\begin{figure}
\centering 

%\includegraphics[width=2.5in]{example-1.pdf} 
\includegraphics[width=\textwidth]{Our-data-2.pdf} 
\caption{Some random example extracted from COCO-text with the visual context information. For each sample, there are a bounding box and full images and information from the image such as object, scene scenario and image description.  }
%Language Model
%thanks to the frequency count dictionary
 \label{fig:survey}
% \label{table_1}
 \end{figure} 










\subsection{Image description}

\textcolor{black}{Finally, we considered a caption generator to extract a natural language description from each image.}   


\textcolor{black}{Image caption generator approaches can be classified in two families : 1) bottom up 2) top down. The bottom up approach consists of detecting objects in the image and then attempting to combine the identified objects into a caption \cite{vinyals2015show}. On the other hand, the top-down approach learns the semantic representation of the image which is then decoded into the caption \cite{karpathy2015deep}. Current state-of-the-art adopts the \textit{top-down} approach, using RNN-based architectures.}
%\cite{donahue2015long,vinyals2015show}
In this work, we follow the second approach, top-down method, deep model to extract the visual description of the image. %The model is pre-trained caption generator, that has encoder CNN to extract the visual information and decoder LSTM that generate the natural language.
%\textcolor{red}{Is the caption generator off-the-shelf?  If so, you should cite it.  If you built it, you should cite which paper you emulated, and which data did you use to train it.}




We use this standard architecture \cite{vinyals2015show} to generate a caption from the image. The caption generator encoder uses \textit{resnet-152} architecture \cite{he2016deep} that trained on ILSVRC competition dataset for general image classification task, and the decoder tuned on COCO-text (same dataset that we use in out experiments). 
%\textcolor{blue}{In particular, the encoder is pre-trained resent152 that is trained on ILSVRC competition dataset for the general image classification task. The decoder is tuned on COCO-text caption dataset, the same dataset we extract other visual context information.}

\textcolor{black}{The encoder CNN extract the fixed length visual feature embedding $W_{v}$ from each input image. Then that feature vector is transformed to have the same dimension to decoder network LSTM $v = W_{v}(CNN(I))$.  The decoder LSTM represent the word with one hot vector $S_{t}$ of the same dimension of the dictionary.  The target texts for each image are predefined as sequence of list containing $S_{0}$ '$<start>$'  and $S_{N}$ '$<end>$' for each sentence. for example, this sequence \textit{a group of people on ski in the snow} converted to list as  ['$<start>$','$a$' ,'$group$' ,'$of$' ,'$people$' ,'$on$', '$ski$', '$in$', '$the$','$snow$','$<end>$' ]. The word embedding parameter $W_{s}$  is $x_{t} = W_{t}S_{t}$ where $t \in {(0,... N-1)}$. Then, the LSTM map the feature $v, x_{t}$ to internal hidden $h_{t}$ and decoded into probability to be able to predict the word in the same state $p_{t}+1 = LSTM(v,x_{t},h_{t})$ where  $t \in {(0,... N-1)}$. In summary, the caption generator can predict these words of the caption in a correct sequence given any image $p(S|I) = p(S_{t}|I, S_{1}, S_{2} ... S_{t-1})$  where $S$ is the caption and $S_{t}$ is location of word in the caption.}  





\section{Evaluation remarks} 
%\lable{sec:Preliminaries}
For evaluation, we used a less restrictive protocol than the standard proposed by \cite{wang2010word} and adopted in most state-of-the-art benchmarks, which does not consider words with less than three characters. This protocol was introduced to overcome the false positives on short words that most current state-of-the-art struggle with, including our Baseline. For instance, current state-of-art model \cite{fang2018attention} evaluate the model without non-alphanumerically characters or irregular text in ICDAR15 dataset, that less complex than COCO-text. Instead, we consider all cases in the dataset, and words with less than three characters are also evaluated.


\section{Challenges and Discussion}
%COCO-text is dataset 
%The end-to-end text recognition of scene  text  in  natural images  with  unconstrained  environments  remains  a  chal-lenging  problem  in  computer  vision.  

According to the author of this dataset, the shortcoming is the re


\begin{figure}
\centering 

%\includegraphics[width=2.5in]{example-1.pdf} 
\includegraphics[width=\textwidth]{wrong-example.pdf} 
\caption{Some random example extracted from COCO-text with poor detection. The poor detection effect the accuracy of the our baseline sdsds sdsdf sfsdf fdfdsfd dssf  checking the space for more  Some random example extracted from COCO-text with poor detection. The poor detection effect the accuracy of the our baseline sdsds sdsdf sfsdf fdfdsfd dssf  checking the space for more   }
%Language Model
%thanks to the frequency count dictionary
 \label{fig:survey}
% \label{table_1}
 \end{figure} 







\section{Human evaluation}
To estimate an upper bound for the results, we picked 33 random pictures from the test dataset and had 16 human subjects try to select the right word among the top $k=5$ candidates produced by the baseline CV system. Our proposed model performance on the same images was 57\%. Average human performance was 63\% (highest 87\%, lowest 39\%).



\begin{table}[h]
\centering
\caption{A human subject performance comparing with our proposed methods.}
\begin{tabular}{c|c|c|c}
\hline 
Subject & Average & Max & Sample \\ 
\hline 
Machine &     51\%   &  57\%   & 528 \\
\hline 
Human &   65.3\%     &     87\% & 528      \\ 
\hline 
\end{tabular}
\end{table}





\begin{figure*}[ht]
\centering 

%\includegraphics[width=2.5in]{example-1.pdf} 
\includegraphics[width=\textwidth]{s.pdf} 
\caption{The user interface  presented to our human subjects
through the survey website asking them to re-rank the text hypothesis based on the visual information }
%Language Model
%thanks to the frequency count dictionary
 \label{fig:survey}
% \label{table_1}
 \end{figure*} 








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Chapter 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%



\chapter{Learning to Re-rank with Word Embedding (word level)}



%\section{Word Emedding}

\section{Overview of the Approach}

%\section{General  Description of our Approach}
%\label{sec:baseline}

Text recognition approaches can be divided in two categories: (a) character based methods that rely on a single character classifier plus some kind of sequence modeling (e.g. n-gram models or LSTMs), and (b) lexicon-based approaches that intend to classify the image as a whole word. 

In both cases, the system can be configured to predict the $k$ most likely words given the input image. Our approach focuses on re-ranking that list using language information such as word frequencies, or semantic relatedness with objects in the image (or \texttt{visual context}) in which the text was located.
%(1) general language information such as word frequencies, and (2) visual context of the image in which the text was located.
\subsection{Baseline Systems}
\label{sec:baselinesystems}
We used two different  off-the-shelf baseline models: First, a CNN \cite{jaderberg2016reading} with fixed lexicon based recognition. The lexicon act as a language prior (dictionary) one word per class, for classification task. The fixed dictionary containing around 90K word forms. 

Second, we considered a LSTM architecture with a visual attention model \cite{ghosh2017visual}. The LSTM generates the final output words as character sequences, without relying on any lexicon. In particular, The model was trained with beam search, over the LSTM output, to preform word inference.   

Both models are trained on a synthetic dataset \cite{jaderberg2014synthetic}. The output of both models is a vector of softmax probabilities for candidate words.
%The baseline is a deep neural network that already trained with huge amount of data, which provides a series of text hypotheses per text images. Our proposed approach is to re-rank that text hypotheses using visual context information. 
For each text image the baseline provides a series of $k$ text hypotheses, that is fed to our model. Let us denote the baseline probability of any of the $k$ most likely words ($w_j, 1\le j\le k$) produced by the baseline as follows: 
\begin{equation} 
%\small {P_0(w) = p(w|\textrm{BL})}
\small \begin{array}{l}P_{BL}(w_j)\;=\;softmax(w_j,BL)\\\\\end{array}
\end{equation}
%\subsection{fasttest}

%\subsection{Glove}

%\subsection{Universal Encoder}


\begin{figure}
\centering 

%\includegraphics[width=2.5in]{example-1.pdf} 
\includegraphics[width=\textwidth]{overview.pdf} 
\caption{overview.}
%Language Model
%thanks to the frequency count dictionary
 %\label{fig:all-data-example}
% \label{table_1}
 \end{figure} 





\section{Language model}

%The language model is based on unigram, which based on language model N-gram.
%\begin{equation}
%%\begin{array}{l}\prod_{i=1}^kP(w_i)\;\\\\\end{array}
%%e.g.,  P(w_1,w_2,w_3) = P(w_1)P(w_2|w_1)P(w_3|w_1 w_2)
%P(w_1w_2...w_n)\;\approx\prod_iP(w_i)
%\end{equation} 
%\smallskip
%\subsubsection*{N-gram}
The formalization of the idea of word prediction with probabilistic models called N-gram, which predict the next word form the previous n-1 words. Computing the probability of the next word will turn out to be closely related to computing sequence of words. N-grams are essential in any task in which we have to identify words in noisy and ambiguous input. For instance, in speech recognition, the input speech are noisy and most of the word extremely similar. In \cite{shilman2004spatial} give an intuition form handwriting recognition for how probability of word sequence can help. In spelling correction, finding the correct spelling error like \cite{kukich1992techniques} that introduced techniques for automatically correcting words in text. Beside, these area N-gram has also an application in part-of-speech tagging, natural language generation, word similarity and as well text prediction system in different systems like smart-phone. The use of N-gram of many applications, such as \cite{shilman2004spatial,kukich1992techniques} is the key inspiration for this language model in this work.  

N-grams are used for a variety of different task. For example, when developing a language model, n-grams are used to develop not just unigram models but also bigram and trigram models. N-gram models that can be used in a variety of tasks such as spelling correction, word breaking and text summarization. In this work, we utilized one of many approaches of N-gram such as unigram based dictionary. The probabilities of the unigram model are computed from a the {\textit{Opensubtitles}}\footnote{\href{https://opensubtitles.org}{https://opensubtitles.org}} \cite{lison2016opensubtitles2016} and {\textit{Google book n-gram}}\footnote{\href{https://books.google.com/ngrams}{ https://books.google.com/ngrams}} 
 text corpora (7 Million token). The main goal of ULM is to increase the probability of the most common words proposed by the baseline. 
\begin{equation}
\small P_{ULM}(w)\;=\;\frac{count(w_j)}{\sum_{w\in C} count(w)} 
\end{equation}  

It is worth mentioning that the language model is very simple to build, train, and adapt to new domains, which opens the possibility of improving baseline performance for specific applications.  

%\subsection{Unigram language model}
%\subsubsection{Evaluation}
%\subsubsection{Perplexity}
\section{Visual Context Information} 
\subsection{Estimating Relatedness from Training Data}

%Another option to compute semantic relatedness is to estimate it from training data. This should overcome word embedding limitation when the candidate word and the image objects are not semantically related in general text, but frequently co-ocurr in the dataset (e.g. commercial brands are commonly seen in sports channels). For this, we use training data to estimate the conditional probability $P_{TDP}(w \vert c)$ of a word $w$ given that object $c$ appears in the image:


A second possibility to compute semantic relatedness is to estimate it from training data. This should overcome the word embedding limitation when the candidate word and the image objects are not semantically related in general text, but are on real world. For instance, as shown in  Figure \ref{fig:SWE}, the sports TV channel \textit{kt} and the object \textit{racket} have no semantic relation according to the word embedding model SWE, but they are found paired multiple times in the training dataset,  which implies they do have a relation.  %96\% 
%For this, we use training data to estimate the conditional probability $P_{TDP}(w \vert c)$ of a word $w$ given that object$c$ appears in the image:
\begin{equation}
\small P_{TDP}(w\vert c)\;=\;\frac{count(w,c)}{count(c)}
\end{equation}
Where $count(w,c)$ is the number of training images where $w$ appears as the gold standard annotation for recognized text, and the object classifier detects object$c$ in the image. Similarly, $count(c)$ is the number of training images where the object classifier detects object $c$.  









%\section{Similarity to probabilities}


%\section{Implementation details}

%\subsection{Signal visual context}
\subsection{Semantic relatedness Using Word embedding}
\textcolor{black}{We compute the word-embedding based semantic relatedness (SWE) in the following steps: First, we use a threshold $\delta$ to eliminate lower probabilities from the visual classifier (objects, scenes). Secondly, following  \cite{lee2018stacked} that shows the beneficial importance of finding the proper visual context of each region in a caption guided by text using semantic similarity,  we compute the embedding-based similarity of each visual with the candidate word. The idea is to match the most semantically related visual context with candidate word.} 
Thirdly, we take the max-highest similarity score, most semantically related, to the candidate word $C_{max}$ as :

\begin{equation}
    c_{max}= \argmax_{%
       \substack{%
         %\text{}\, c_i \large \epsilon vci\\
         \phantom{\text{s.\,t.}}\, c_i \in Image \\  
         \phantom{\text{s.\,t.}}\, P(c_i) \geq \delta   
       }
     }
     sim(w,c_i) %\mathrm{~~~~~where~} \beta = threshold 
   \end{equation}

  Finally, following \cite{blok2003probability} with confirmation assumption $p(w|c) \geqslant p(w)$, we compute the conditional probability from similarity as:
\begin{equation}
\label{eq:sim}
~~~~~~~P_{SWE}(w\vert c_{max})=P(w)^\alpha
\end{equation}
\noindent where:
\begin{itemize}
\itemsep 0cm
\item $\alpha=\left({\textstyle\frac{1-sim(w,c_{max})}{1+sim(w,c_{max})}}\right)^{1-P(c_{max})}$
\item $P(w)$ is the probability of the word in general language (i.e. a unigram model) computed from {\textit{Opensubtitles}} \cite{lison2016opensubtitles2016} and {\textit{Google book n-gram}}\footnote{\href{https://books.google.com/ngrams}{ https://books.google.com/ngrams}} text corpora (7 million token).
\item $P(c_{max})$ is the probability (as produced by the visual classifier) of the context object/scenario most semantically related to the candidate word.
\end{itemize}







\begin{figure*}[t!]
\centering 
%\includegraphics[width=3.0in]
%\includegraphics[width=0.8\textwidth]{f8.pdf}
%\includegraphics[width=0.8\textwidth]{f9.pdf}
\includegraphics[width=0.8\textwidth]{accv-3c.pdf}
%\vspace{-2mm}
%\vspace{-7mm}
\caption{Scheme of the proposed visual context information pipeline integration into the text spotting system. Our approach uses the language model and a semantic relatedness measure to re-rank the word hypothesis. The re-ranked word \textit{quarters} is semantically related with the top ranked visual \textit{parking}. }%See more examples in Figure \ref{fig:moreexamples}.}
\label{fig:happy} 
\end{figure*}




\subsection{Semantic relatedness Using Dual Word embedding}

The SWE reranker described in section \ref{sec:swe} uses separately context information from the object or scene classifiers. The reranker described in this section aims to combine both kinds of information in a single step.
%Finally, following \cite{blok2003probability} with confirmation assumption $p(w|c) \geqslant p(w)$, we convert the obtained similarity to a conditional probability as:
Also, following  the same confirmation assumption $p(w|c) \geqslant p(w)$, we convert the obtained similarity to a conditional probability as:
 \begin{equation}
\begin{split}
\label{eq:sim2}
& P_{DSWE}(w\vert c_{max_{1}}, c_{max_{2}})= \beta M +(1- \beta)S, \\
\mathrm{where:}& \\
& M = max(P_{SWE}(w|c_{max_1}), P_{SWE}(w|c_{max_2}))\\
& S = P_{SWE}(w|c_{max_1})+P_{SWE}(w|c_{max_2})\\
& \:\:\:\:\:\:\:\:-P_{SWE}(w|c_{max_1})\times P_{SWE}(w|c_{max_2})\\
& \beta = max
\begin{cases}
\text{$sim(c_{max_{1}},c_{max_{2}})$}\\
\text{$sim(w, c_{max_{1}}$)}\\
\text{$sim(w, c_{max_{2}}$)} \\ 
\text{$1 - sim(w, c_{max_{1}})$} \\ 
\text{$1 - sim(w, c_{max_{2}})$} \\ 
\text{$P_{SWE}(w|c_{max_1})$} \\
\text{$P_{SWE}(w|c_{max_2})$} \\ 
%\text{$sim(c_{max_{1}},c_{max_{2}})$}, \\ 
%\text{$sim(w, c_{max_{1}}$)},
%\text{$sim(w, c_{max_{2}}$)}, \\ 
%\text{$1 - sim(w, c_{max_{1}})$},  
%\text{$1 - sim(w, c_{max_{2}})$}, \\ 
%\text{$P_{SWE}(w|c_{max_1})$}, 
%\text{$P_{SWE}(w|c_{max_2})$}
\end{cases}      
\end{split}
\end{equation}


Note that Equations \ref{eq:sim} and \ref{eq:sim2} already includes frequency information from the ULM, therefore it is taking into account not only the semantic relatedness, but also the word frequency information used in the ULM re-ranker above. Also, if there is no available visual context information, we back-off to $\alpha=1$ and use the bare unigram probability.



\begin{figure}
\centering 
%\includegraphics[width=3.5in]{word2vec-skip-gram.png}
\includegraphics[width=\textwidth]{parking-example.png} 
\caption{Figure 4.3 output visualisation }
\end{figure}





\subsection{Semantic Relatedness Using Word Embeddings from Training Data (TWE)}
%This re-ranker builds upon a word embedding, as the SWE re-ranker above, but the embeddings are learnt from the training dataset (considering two-word ``sentences'' consisting of the target word and the object/scene in the image). The embeddings can be computed from scratch, using only the training dataset information (TWE).   
%\textcolor{red}{it is not clear if the embeddings have just one output word (object), or two (object and place) (Prof., the formula is very clear that one visual context}  
This re-ranker builds upon a word embedding, as the SWE re-ranker above, but the embeddings are learnt from the training dataset (considering two-word ``sentences'' consisting of the target word and the object/scene in the image). The embeddings can be computed from scratch, using only the training dataset information (TWE). At inference time we only consider one visual context at a time similar to SWE.   
%\textcolor{red}{it is not clear if the embeddings have just one output word (object), or two (object and place) (Prof., the formula is very clear that one visual cont



\textcolor{black}{In particular, we train a skip-gram model, with negative sampling loss, to predict the \textit{context} $w_{c}$ word when given a center word $w$. In our case, the context word is the visual information, meanwhile the given word is the text hypothesis. For example as in Figure \ref{fig:bike}
the context word is \textit{street} or \textit{unicycle} given to input word \textit{bike} as common word between the two contexts in the two-word sentences with one window size as: } \\ 
%\textcolor{red}{this picture suggests ''sentences'' of three words for the embeddings, not two}  


\begin{minipage}[c]{1\textwidth}
    \centering
\tikzstyle{every picture}+=[remember picture]

\tikzstyle{na} = [shape=rectangle,inner sep=0pt,text depth=0pt]
\tikz\node[na] (word1){street};\tikz\node[na](word2){\bf{}}; 
\tikz\node[na] (word2){\bf{bike}}; \tikz\node[na](word3){unicycle}; 
%%\tikz\node[na](word1){kia};  \tikz\node[na](word2){t};
\begin{tikzpicture}[overlay]
\centering
  \path[->,red,thick](word2) edge [out=90, in=90] (word1);  
    \path[->,black,thick](word2) edge [out=90, in=90] (word3);  
  %\tikz\node[na](word1){John}; loves \tikz\node[na](word2){his}; mother.
\end{tikzpicture}

\end{minipage}

In this case, we convert the similarity produced by the embeddings to probabilities using:





%\subsection{datasets}

%\subsection{Implementation Details}
%\subsubsection{Signal visual context}
%We compute the word-embedding based semantic relatedness (SWE) in the following steps: First, we use a threshold $\delta$ to eliminate lower probabilities from the visual classifier (objects, scenes). Secondly, following  \cite{lee2018stacked} that shows the beneficial importance of finding the proper visual context of each region in a caption guided by text using semantic similarity,  we compute the embedding-based similarity of each visual with the candidate word. 












\subsection{Combining re-ranker}

Our re-ranking approach consists in taking the softmax probabilities computed by the baseline DNN and combine them with the probabilities produced by the re-ranker methods described in Section~\ref{sec:reranking}. We combine them by simple multiplication, which allows us to combine any number of re-rankers in cascade. 
For simplicity, in the equations below $C$ stands for the set of available context information (i.e., object, scenario, and caption). We evaluated the following combinations:

\begin{enumerate}
\item The baseline output is re-ranked by the unigram language model:
\begin{equation}
%\small P_2(w)=P(w\vert \textrm{BL})\times P(w\vert \textrm{VCI})
\small P_0(w)=P_{BL}(w)\times P_{ULM}(w)
\label{eq:ULM}
%\tag{5}
\end{equation}

\item  The baseline output is re-ranked by the relatedness estimated from the training dataset as conditional probabilities (TDP).  
\begin{equation}
%\small P_2(w)=P(w\vert \textrm{BL})\times P(w\vert \textrm{VCI})
\small P_1(w,c)=P_{BL}(w)\times P_{TDP}(w\vert C)
\label{eq:TDP}
%\tag{5}
\end{equation}

\item The baseline output is re-ranked by the general word-embedding model (SWE/DSWE). Note that this reranker also includes the ULM information.
\begin{equation}
%\small P_2(w)=P(w\vert \textrm{BL})\times P(w\vert \textrm{VCI})
\small P_2(w,c)=P_{BL}(w)\times P_{SWE/DSWE}(w\vert C)
\label{eq:SWE}
%\tag{5}
\end{equation}


\item  The baseline output is re-ranked by the word-embedding model trained entirely on training data (TWE) :
\begin{equation}
%\small P_2(w)=P(w\vert \textrm{BL})\times P(w\vert \textrm{VCI})
\small P_3(w,c)=P_{BL}(w)\times P_{TWE}(w\vert C)
\label{eq:TWE}
%\tag{5}
\end{equation}

\item The baseline output is re-ranked by SWE/DSWE general  word  embedding  and TDP re-rankers combined: 
\begin{equation} \label{eq:SWE+TDP}
%\small {P_5(w)= p(w|\textrm{BL})\times p(w|\textrm{VCI})\times p(w|\textrm{VCI2})}
\small P_4(w,c)=P_{BL}(w)\times P_{SWE/DSWE}(w\vert C)\times P_{TDP}(w\vert C)
%\small P_5(w,c)\;=\;P_{BL}(w)\;\times\;P_{SWE}(w\vert c)\;\times P_{TDP}(w\vert c)
\end{equation}

\item The combination of TDP and TWE (with or without ULM):
%\begin{equation} 
\begin{gather}\label{eq:TDP+TWE}
%\small {P_5(w)= p(w|\textrm{BL})\times p(w|\textrm{VCI})\times p(w|\textrm{VCI2})}
\small P_5(w,c)=P_{BL}(w)\times P_{TDP}(w\vert C)\times P_{TWE}(w\vert C) \\ 
\small P_{5a}(w,c)=P_{BL}(w) \times P_{ULM}(w) \times P_{TDP}(w\vert C) \times P_{TWE}(w\vert C)
\end{gather}




%\item The baseline output is re-ranked by Neural re-ranker (FCNN) (with or without ULM):
%\begin{gather}
%\small P_6(w)=P_{BL}(w) \times P_{FCNN}(w\vert C) \\
%\small P_{6a}(w)=P_{BL}(w) \times P_{ULM}(w) \times P_{FCNN}(w\vert C) \;
%\end{gather}

%\item  The baseline output is re-ranked by the word-embedding model trained entirely on training data (TWE) and Neural re-ranker (FCNN) %re-rankers combined :
%\begin{equation}
%\small P_7(w)=P_{BL}(w) \times P_{FCNN}(w\vert C) \times P_{TWE}(w\vert C)
%\end{equation}


%\item The baseline output is re-ranked by Neural re-ranker (FCNN) \\  and (TDP) re-rankers combined  (with or without ULM) :
%\begin{gather}

%\small P_8(w)=P_{BL}(w) \times P_{TDP} (w\vert C) \times P_{FCNN}(w\vert C) \\ 
%\small P_{8a}(w)=P_{BL}(w)  \times P_{ULM}(w) \times P_{TDP} (w\vert C)  \times P_{FCNN} (w\vert C)
%\end{gather}
%\item  The baseline output is re-ranked  Neural re-ranker (FCNN) and general word embedding (SWE/DSWE) re-rankers combined :

%\begin{equation}
%\small P_9(w)=P_{BL}(w)\times P_{FCNN}(w\vert C) \times P_{SWE/DSWE}(w\vert C)
%\label{eq:FCNN-SWE}

%\end{equation}

%\item  The baseline output is re-ranked by Neural re-ranker (FCNN), general word embedding (SWE/DSWE) and (TDP) re-rankers combined :  \\ 
%\begin{equation}
%\small P_2(w)=P(w\vert \textrm{BL})\times P(w\vert \textrm{VCI})
%\small P_{10}(w)=P_{BL}(w) \times P_{TDP}(w\vert C) \times P_{FCNN}(w\vert C) \times P_{SWE/DSWE}(w\vert C) 
%\label{eq:FCNN-all}
%\end{equation}

%\item  The baseline output is re-ranked by Neural re-ranker (FCNN) and the word-embedding model trained entirely on training data (TWE) and  \\ 
 %(TDP) re-rankers combined :
%\begin{equation}

%\small P_{11}(w)=P_{BL}(w)  \times P_{TDP} (w\vert C) \times P_{FCNN}(w\vert C)  \times P_{TWE}(w\vert C)
%\label{eq:FCNN-TWE}
%\tag{5}
%\end{equation}

\end{enumerate}






\section{Experiments}




\subsection{Experimental with language model}
As a proof of concept, we trained our unigram language model on two different copora. The first ULM was trained on {\textit{Opensubtitles}\cmt{\footnote{\href{https://www.opensubtitles.org}{\textit{https://www.opensubtitles.org}}}}, a large database of subtitles for movies containing around 3 million word types. \textcolor{black}{Opensubtitle corps contain a variety of different text such numbers, colloquialism, non-standard word and alphanumeric character that make it well suited for our task.}
 %including numbers and other alphanumeric combinations
Secondly, we trained another model with {\textit{Google book n-gram}}\cmt{\footnote{\href{https://books.google.com/ngrams}{\textit{https://books.google.com/ngrams}}}}, that contains 5 million word types from American-British literature books. We used this corpus to increase the frequency of most common word that not seen in the other corpus. In addition, this corpus alone is not sufficient for our task. Thus, we combine both corpora that contains around 7 million word types (token). 

In this experiment, we extract the $k=2,\ldots,9$ most likely words --and their probabilities-- from the baselines. Although the sequential nature of the LSTM baseline captures a character-based language model, our post-process uses word-level probabilities to re-rank the word as a whole (most current state-of-the-arte use word level approach). Note that since our baselines work on cropped words, we do not evaluate the whole end-to-end but only the influence of adding external knowledge.  
% end-to-end system 





\begin{table}[t]
\begin{center}
\caption{Examples of $P(word|object)$ for each re-ranker. TDP and TWE capture relevant information to improve the baseline for pairs word-object/scene that appear in the training dataset. The TPD overcome word-embedding limitation in samples happen in training datasets.}

\begin{threeparttable}

\begin{tabular}{|l|l|l|l|l|l|}
%\hline \bf \small word & \ \small w1   & \small p1     & \small w3  &  \small p3 \\ 
\hline 
 %Word & Visual   &    VCI$_1a$ &  VCI$_1b$ &  VCI$_2$ &  VCI$_3$ &  VCI$_3$*  \\
 Word &  Visual    &  SWE &  TDP &  TWE &  TWE* \\
\hline \hline
delta     & airliner &  0.0028  & \textbf{0.0398}  & 0.0003  &  0.00029     \\
%quarters  & parking  &  0.00023  & 0.0023  & 0.00347 & 0.00023 &  0.00023        \\ 
kt         & racket  & 0.0004 &   \textbf{0.0187} &   0.0002 & 0.00006 \\ 
%5        & ice       &   0.0033 &  0.00898  & \textbf{0.03341} &           0.0160  \\ 
plate     & moving   &  0.0129  & 0.00050  & \textbf{0.326}  &  0.00098       \\
way       & street   & 0.1740    & 0.02165  &   \textbf{0.177}       &  0.17493  \\ 
%air       & wall       & 0.0343  & 0.00050  & \textbf{0.04882}  &  0.03368        \\ 
\hline
\end{tabular}




\end{threeparttable}
\end{center}

 \label{table_3} 
 \end{table}



The first baseline is a CNN \cite{Max:16} with fixed-lexicon recognition, which is not able to recognize any word outside its dictionary. The results are reported in Table \ref{table_1}. We present two different accuracy metrics: \textit{full} columns correspond to the accuracy on the whole dataset, while \textit{dictionary} columns correspond to the accuracy over the solvable cases (i.e. those where the target word is among the 90K-words of the CNN dictionary, which correspond to 43.3\% of the whole dataset). 
We also provide the results using different numbers of $k$-best candidates. Table \ref{table_1} top  row shows the performance of the CNN baseline, and the second row reports the influence of the ULM. The best result is obtained with $k=7$, which improved the baseline model in 2.2\% \textit{full}, 6.4\%  \textit{dictionary} and retrieve 75.3\% from the correct text hypothesis.
}

The second  baseline we consider is an LSTM \cite{Suman:17} with visual soft-attention mechanism, performing unconstrained text recognition without relying on a lexicon. The first row in Table \ref{table_1} reports the LSTM baseline result on this dataset, and the second row shows the results after the ULM re-ranking. The best results are obtained by considering $k=4$ which improves the baseline in 1.7\% \textit{full} and 76.2 \% retrieval score.

\subsection{Experimental with visual information}

%\subsection{\textcolor{black}{Experiments with Visual Context Information }}
%\label{experiment2}

The main contribution of this paper consists in  re-ranking the $k$ most likely hypotheses candidate word using the visual context information. Thus, we use ICDAR-2017-Task3 dataset to evaluate our approach, re-ranking the baseline output using the semantic relation between the spotted text in the image and its visual context.  
%We next re-rank the most probable word based on visual context information. We use the official ``COCO-Text ICDAR 2017 end-to-end reading robust competition'' dataset. 
As in the language model experiment, we used ground-truth bounding boxes as input to the BL. However, in this case, the whole image is used as input to the visual classifier.
 
 
In order to extract the visual context information we considered two different pre-trained state-of-the-art visual classifiers: object and scene classifiers. For image classification we rely on three pre-traind network:   
ResNet \cite{Kaiming:16}, GoogLeNet \cite{Szegedy15} and Inception-ResNet-v2 \cite{szegedy2017inception}, all of them able to detect pre-defined list of 1,000 object classes. However, for testing we considered only Inception-ResNet-v2 due to better \textit{top-5 accuracy}. For scene classification we use places classifier \textit{Place365-ResNet152} \cite{zhou2017places} that able to detect 365 scene categories.

%\textcolor{black}{On the other hand, we use a caption generator that use a encoder and decoder to generate the synthetic natural language description . The encoder  was trained on ILSVRC competition dataset for general image classification task, and the decoder is tuned on COCO-text (same dataset). The encoder extract the visual feature from each  input image. Then, that feature vector is transformed to have the same dimension to decoder network (LSTM) that generate text, same as show and tell architecture \cite{vinyals2015show}.}  %The target texts for each are predefined as sequence of list containing '$<start>$'  and '$<end>$' for each sentence. for example, this sequence \textit{a group of people on ski in the snow} converted to list as  ['$<start>$','$a$' ,'$group$' ,'$of$' ,'$people$' ,'$on$', '$ski$', '$in$', '$the$','$snow$','$<end>$' ].

Although the visual classifiers use a softmax to produces only one probable object hypotheses per image, we use \textit{threshold} to extract a number of object-scene hypotheses, and  eliminate  low-confidence results. Then, we compute the semantic relatedness for each object-scene hypotheses with the spotted text. Finally, we take the most related visual context. The caption generator we use only top-k for extracting the image description. However, this approach can extended to multiple captions.      


In this experiment we re-rank the baseline $k$-best hypotheses based on their relatedness with the 1) objects 2) scene 3) synthetic natural language description (caption) in the image. We try three approaches for that: 1) semantic similarity computed using word embeddings \cite{mikolov2013distributed} and 2) correlation based on co-ocurrence of text and image object in the training data. 3) semantic similarity from image description with  deep nueral network. 
%semantic similarity with the word produced by the object classifier 2) training dataset, the conditional probability between text image happen together in the same dataset.

First, we re-rank the words based on their word embedding: semantic relatedness with multiple visual context from general text : 1) object  (SWE$_{object}$) and 2) scene (SWE$_{place}$). For instance, the top example in Figure~\ref{fig:SWE}  shows that the strong semantic similarity between scene information \textit{pay} and \textit{parking} re-ranked that word from 3rd to 1st position. We tested three pre-trained models trained on general text as baseline 1) word2vec model with 100 billion tokens 2) glove model with 840 billion tokens \cite{pennington2014glove} and 3) fastText with 600 billion tokens. However, we adopt glove as baseline, due to a better similarity score. 
 %\cite{bojanowski2017enriching}
\textcolor{black}{Secondly, we re-rank words based on a combined SWE (objects , places) in a single step (Dual SWE). DSWE uses two visual information (object, scene) to compute the relation between the two visual contexts to the candidate words. For example, in Table \ref{tab:exam2} DSWE was able to re-rank the candidate word \textbf{dunkin}, donuts store, based on detected context \textit{coffee} and \textit{bakery}. Also, we use  DSWE with two objects from different classifier Inception-ResNet-v2 \cite{szegedy2017inception} and Resnet152 \cite{Kaiming:16}. In addition, extracting top-2$k$ words from the same classifier preform better in case of DSWE$_{place+place}$. } 

\textcolor{black}{In particular, results in Table \ref{table_1} show that DSWE alone improves the CNN accuracy. However, SWE (single embedding) yields better performance on both baselines, since DSWE struggles with short words and false positives. For instance, as shown in Table \ref{tab:exam2}, the candidate one-letter word \textbf{w} has low semantic relatedness score with visual contexts \textit{racket} and \textit{stadium}.}

%\vspace{-2mm}
%\vspace{-7mm}

\begin{figure*}[t]
\centering 

\includegraphics[width=4.6in]{example-c2.eps}

\caption{Some examples of visual context re-ranker. The top-two examples are successful results of the visual context re-ranker. The top-left example is a re-ranking results based on the relation between text and its visuals happen together in the training dataset. The top-right is a re-ranking result based on semantic relatedness between the text image and its visual. The bottom two cases are examples of words either has no semantic correlation with the visual or exist in the training dataset. Not to mention that the top ranking visual $c_1$ is the most semantically related visual context to the spotted text. (Bold font words indicate the ground truth)}

\label{fig:moreexamples} 
\end{figure*}


\begin{table}
\centering
\caption{Comparison of different model in a word level re-ranking scenario}
\begin{tabular}{l|c|c|c|c|c|c}
\hline 
 Model  &  CNN &   $k$ &   list &  LSTM &   $k$  &  list \\
\hline \hline 
 Baseline &    19.7 &  - & - & 17.9 &  -&  -  \\ 
\hline 
 BL+glove       &    22.0   &  7 &  75.8 &    19.1 &  4  &   75.3 \\ 
 \hline
 BL+fasttext    &    21.9   &  7 &  75.4 &    19.4 &  4  &   76.1\\ 
 \hline 
 BL+USE-T$_{word}$    &    21.9    & 6 &    \textbf{77.9} &      19.1 &  4 &  76.3 \\ 
 \hline 
 BL+Bert$_{word}$  &    21.8    & 6 &    77.5 &      18.9 &  4 &  74.7 \\
 \hline 
 BL+TWE  (ours)       &    \textbf{22.2}   &  7 &  76.3  &  19.5  &  4  &   76.7\\ 
%\hline
%MRR BL+Bert$_{w}$ 21.8 ---45.4


% USE-T$_{sentence}$  &     \textbf{22.0}  &  6 &    \textbf{19.2}  &  4 \\%  &  \\      87\% & 462      \\ 
%\hline
%  USE-T$_{sentence+word}$  &    21.9 &  6  &    19.1 &  4 \\% 


\hline 
\end{tabular}
\label{result}
\end{table}
 








Thirdly, we use the training data to compute the conditional probabilities between text image and object in the image happen together (TDP). We also combined both relatedness measures as described in Equation~\ref{eq:SWE+TDP}, obtaining a higher accuracy improvement on both baselines, as can be seen in Table \ref{table_1}, (SWE+TDP) boosted the accuracy for both baseline. For example, as shown in Figure ~\ref{fig:SWE}  the \textit{kt} (sport channel) happens often with visual context \textit{racket}, something that can not be captured by general word embedding models. Also, scene classifier SWE$_{place}$+TDP boost the baseline. The scene classifier SWE$_{place}$ perform better than the object classifier in instance outdoor. For instance, the spotted text in a signboard \textit{way} is more semantically related with \textit{downtown} than a man holding \textit{an umbrella} in the image.   

\textcolor{black}{Fourthly, we trained a word embedding model using the training dataset (TWE). Due to the dataset is too small, we use skip-gram model with one window, and without any word filtering. The result is 300-dimension vector for about 10K words. The result in Table \ref{table_1} CNN shows that the combination model TDP+TWE also significantly boost the accuracy up to 7.8\% dictionary, 2.8\% \textit{all} and 77.3\% retrieval. Also, the second baseline LSTM accuracy boosted up to 1.8 \% and 77.5 retrieval. Not to mention that TDP+TWE model only rely on the visual context information, computed by Equation \ref{eq:tanh}.} 

Finally,  We also compare our result with current state-of-the-art word embeddings trained on a large general text using  \textit{glove} and \textit{fasttext}. We experimental with Universal Sentence Encoder (USE) current State-of-the-art in Semantic Textual Similarity (STS) with fine tuning and extracting feature to compute the semantic relation with cosine distance. The USE-T$_{word}$ is require a shorter hypothesis list, less $k$, to get the top performance when the right word is in the hypothesis list (better retrieval score than the other methods). 
















\begin{table}[t]

\begin{center}
\caption{Examples of $P(word|object+scene)$ for each re-ranker. DSWE capture more relevant information to improve the baseline from both object and scene information.}
\label{tab:exam2} 
\begin{threeparttable}

\begin{tabular}{l|l|l|l|l|l}
%\hline \bf \small word & \ \small w1   & \small p1     & \small w3  &  \small p3 \\ 
\hline 
 %Word & Visual   &    VCI$_1a$ &  VCI$_1b$ &  VCI$_2$ &  VCI$_3$ &  VCI$_3$*  \\
 \textbf{Word} &  \textbf{object}  &  \textbf{place} &  SWE$_{object}$ & SWE$_{place}$ & DSWE \\
\hline \hline
dunkin    & \bf{\textcolor{black}{coffee}} &  \bf{\textcolor{black}{bakery}}  &   0.0250 &  0.0142  & \bf{\textcolor{black}{0.0309}}   \\
\hline
way       & \bf{\textcolor{black}{street}} &   \bf{\textcolor{black}{hospital}}  & 0.1771 &   0.3425 & \bf{\textcolor{black}{0.3787}}   \\
\hline
canada    & \bf{\textcolor{black}{passenger}} &  \bf{\textcolor{black}{bus}}  &0.0004   &  0.0001  & \bf{\textcolor{black}{0.0005}}   \\
%airway    & airliner & airfield & 0.0012 & \textbf{0.0029} & 0.0025 \\ 
\hline
member    &  \bf{\textcolor{black}{ballplayer}}  & \bf{\textcolor{black}{baseball}} & 0.0006 & 0.0009 & \bf{\textcolor{black}{0.0011}} \\
\hline
bank      &  \bf{\textcolor{black}{traffic}} & \bf{\textcolor{black}{motel}}  &  0.0112 &  0.1079  & \bf{\textcolor{black}{0.1361}}  \\
\hline
3         &   \bf{\textcolor{black}{ballplayer}} & baseball &   \bf{\textcolor{black}{0.0075}} & 0.002 & 0.0072 \\ 
\hline 
w         &   racket &  \bf{\textcolor{black}{stadium}}   &  $9.88\text{e-}5$ & $\bm{2.53\text{e-}4}$ & $2.51\text{e-}4$ \\ 
\hline 
sony      & \bf{\textcolor{black}{racket}} &  athletic   & \bf{\textcolor{black}{0.0154}}  &   $1.09\text{e-}4$&  $1.05\text{e-}4$  \\ 

%0.000109 
%\color{red}{test}    & \textcolor{blue}{racket} &  athletic   & \bf{\textcolor{blue}{0.0154}}  &  0.000109 & $1.7\text{e-}4$ \\ 
%copyright &   airship &  industrial  &   0.0007 &  \textbf{0.00495}  & 0.00492   \\
   
%% & sir  & $1.7\text{e-}4$ & \textbf{star} & \textbf{$\textbf{3.1\text{e-}5}$} & sow &

%%%%%%%%%%%%%%%%%%%%

%quarters  & parking  &  0.00023  & 0.0023  & 0.00347 & 0.00023 &  0.00023        \\ %         & racket  & 0.0004 &   \textbf{0.0187} \\ 
%5        & ice       &   0.0033 &  0.00898  & \textbf{0.03341} &           0.0160  \\ 
%plate     & moving   &  0.0129  & 0.00050      \\
%       & street   & 0.1740    & 0.02165   \\ 
%air       & wall       & 0.0343  & 0.00050  & \textbf{0.04882}  &  0.03368        \\ 
\hline
\end{tabular}

\end{threeparttable}
\end{center}


 \end{table}



\begin{figure*}[t]
\centering 

\includegraphics[width=\textwidth]{DSWE-example.pdf}

\caption{Som. Please refer to the table for more example}

\label{fig:moreexamples} 
\end{figure*}





\begin{table}%[h]
\caption{Best results after re-ranking using different re-ranker combinations, and different values for $k$-best hypotheses extracted from the baseline output.} 
\centering
%\caption{Results of re-ranking the $k$-best hypotheses of the baselines on ICDAR-2017-Task3 dataset (\%)} 
\resizebox{12cm}{!}{ % not SURE IF I'm allow to do that 
\begin{tabular}{|l|c c c c|c| c c |c |} 
 
\hline
\textbf{(Re-ranker) Model}  & \multicolumn{4}{c|}{\textbf{CNN}}   & \multicolumn{3}{c|}{\textbf{LSTM}}  \\ 
 %\hline  

  & \textit{full} & \textit{dict} & \textit{list} &  \textit{k} &    \textit{full} &  \textit{list} & \textit{k} \\ 
%%%%\hline\hline only with big table 
\hline
Baseline (BL)  & \multicolumn{4}{c|}{\textbf{full: 19.7 dict: 56.0}}   & \multicolumn{3}{c|}{\textbf{full: 17.9}}  \\
\hline 

%\hline only when the big table

(P$_{0}$) BL+LM$_{\text{7M}}$ &   21.9  & 62.4 &  75.2 & 7  & 19.3 & 76.2 & 4  \\ 
\hline 
%\small BL+\textcolor{red}{TDP$_{objects?places?}$} & ??? & ??? & ??? & ??? & ??? & ??? & ??? \\ 
\textcolor{black}{(P$_{1}$) BL+TDP$_{objects}$} & 20.4 &  58.0 & 77.8 & 4 & 18.8 & 74.0 & 4 \\
%\small BL+\textcolor{red}{TDP$_{objects}$} & 22.7 & 63.3 & 86.9 & 3 & 20.0 & 65.6 & 9 \\
\hline 
 


(P$_{2}$) BL+SWE$_{object}$   & 21.9 &   62.4 &  \textbf{86.2}  & 4 &   19.1 &   75.3  &     4  \\
 (P$_{2}$) BL+SWE$_{place}$   & 22.0 &  62.5   &  75.8 & 7 &   18.7 &   73.6  &     4   \\ 
\hline 
(P$_{4}$) BL+TDP$_{objects}$+SWE$_{object}$  &  22.2 &  63.2 & 81.4 & 5 & 19.5 &  76.9 &  4  \\ 




(P$_{4}$) BL+TDP$_{objects}$+SWE$_{place}$  &   22.3  &   63.5  & 76.9  &  7  & 19.8  &  66.4  & 9 \\ 


\hline 
%(P$_{0}$) BL+TDP$_{objects}$+LM+SWE$_{object}$  &  22.0  & 62.5   & 84.0  & 4  & 0  & 0  & 0 \\ 
%(P$_{0}$) BL+TDP$_{objects}$+LM+SWE$_{place}$   &  22.1  &   62.9 &  84.4 & 4  & 0  & 0  & 0 \\ 
%\hline 





\textcolor{black}{(P$_{2}$) BL+DSWE$_{object+object}$}  &  21.9  &  62.4  &  78.1  & 6  & 19.0 & 74.9 & 4   \\
\textcolor{black}{(P$_{2}$) BL+DSWE$_{place+place}$}   &   21.9  &   62.2 & 75.4 &  7  & 18.4  & 72.5 & 4 \\
\textcolor{black}{(P$_{2}$) BL+DSWE$_{object+place}$}   &   21.9  &  62.2   & 75.4 &  7  & 19.3  & 76.0 & 4 \\

\hline  


\textcolor{black}{(P$_{4}$) BL+TDP$_{objects}$+DSWE$_{object+object}$}       &   22.2   &   63.0 & 84.6 & 4   & 19.5    & 76.9 & 4 \\
 
\textcolor{black}{(P$_{4}$) BL+TDP$_{objects}$+DSWE$_{place+place}$}         &   22.2     &      63.0 &     76.3 &  7 &   19.1  & 75.3 & 4   \\
 
\textcolor{black}{(P$_{4}$) BL+TDP$_{objects}$+DSWE$_{object+place}$}       &   22.2    &   63.0 &  81.2 & 5 & 19.6  & 77.3 &  4  \\ 
% \hline 
% \textcolor{black}{(P$_{2}$) BL+TDP$_{objects}$+LM+DSWE$_{object+object}$}   &   21.9   &   62.4 & 83.8 & 4   & 0    & 0 & 0 \\
 
% \textcolor{black}{(P$_{2}$) BL+TDP$_{objects}$+LM+DSWE$_{place+place}$}       &   22.0    &   62.5 &  84.0 &   4 & 0  & 0 &  0  \\ 
 
%\textcolor{black}{(P$_{2}$) BL+TDP$_{objects}$+LM+DSWE$_{place+place}$}         &   22.1    &      62.4 &     83.8 &  4 &   0  & 0 & 0   \\
 
 
\hline 
(P$_{3}$) BL+TWE$_{object}$  &   22.2  &  76.3  & 63.0 & 7 & 19.5 & 76.9  & 4  \\ 
(P$_{5}$) BL+TDP$_{object}$+TWE$_{object}$  &   \textbf{22.5}  &   63.8  &  77.3 & 7 & 19.7 & 77.5  & 4  \\ 
(P$_{5a}$) BL+TDP$_{object}$+LM+TWE$_{object}$  &   22.1  &   62.9 &  84.4 & 4 & 19.3  & 76.0  & 4  \\ 
 

 
 
%\small BL+\textcolor{red}{TDP$_{objects?places?}$}+MAH$_{caption}$   &  21.5   &  59.9  &  73.3 & 7 & 19.4 & 64.9 & 9\\ 
%\textcolor{blue}{\small(P$_{3}$) BL+TDP$_{objects}$+ULM+WN$_{caption}$}   &  22.5   &  62.5  &  76.5  & 7  & 20.0 & 70.8 & 6  \\ 
%(P$_{3}$) BL+TDP$_{objects}$+MAH$_{caption}$   &  21.5   &  59.9  &  73.3 & 7 & 19.4 & 64.9 & 9\\ 

%\small LSTM-M$_{ULM}$   &  22.8   &  00  &  00 & 0 & 20.5 & 0 & 9\\
%\hline 
%\textcolor{black}{(P$_{6}$) BL+FCNN$_{all}$}   &  22.2      &   63.2   &  75.0   &  8  & 19.8  & 66.4 & 9 \\
%\textcolor{black}{(P$_{8}$) BL+TDP$_{objects}$+FCNN$_{all}$}   &   22.1      &   62.7  &  74.4   &  8  & 20.2  &  75.4 & 5   \\
%\textcolor{black}{(P$_{6a}$) BL+LM+FCNN$_{all}$}   &  22.4      &   63.7   &  82.0   &  5  & 19.6  & 77.3 & 4  \\
%\textcolor{black}{(P$_{8a}$) BL+TDP$_{objects}$+LM+FCNN$_{all}$}   &   22.5     &   64.0   &  75.9   &  8  & 19.7  & 73.5 & 5  \\

%\textcolor{teal}{(P$_{3}$) BL+TDP$_{objects}$+FCNN$_{mask}$}   &   0     &   0   &  0   &  0  & 0  & 0 & 0  \\
%\textcolor{black}{(P$_{3}$) BL+TDP$_{objects}$+TOW$^{\mathrm{*}}$$_{caption}$}   &   23.1      &   64.3   &  76.8   &  8  & \textbf{20.9}  & \textbf{70.9} & \textbf{8}  \\ 
%\hline 

%\textcolor{blue}{\small (P$_{6}$) BL+TDP$_{objects}$+ULM+WN$_{caption}$+SWE$_{places}$}   &   22.5      &   62.6  &  86.0  & 3  & 19.9  & 81.1 & 4 \\ 
%\small BL+\textcolor{red}{TDP$_{objects?places?}$}+MAH$_{caption}$+SWE$_{places}$  &  22.8   &  63.5  &77.8   & 7 & 20.2  & 67.7   &  9 \\  
%(P$_{5}$) BL+TDP$_{objects}$+MAH$_{caption}$+SWE$_{places}$  &  22.8   &  63.5  &77.8   & 7 & 20.2  & 67.7   &  9 \\  
%\small BL+\textcolor{red}{TDP$_{objects?places?}$}+TOW$_{caption}$+SWE$_{places}$   &   23.2      &   64.4   &  77.3  & 8   & 20.8  & 69.7 & 8  \\ 

%\textcolor{black}{(P$_{9}$) BL+FCNN$_{all}$+SWE$_{object}$}   &   22.3 &   63.3   &  81.6  & 5   & 19.3  & 76.0 & 4  \\ 
%\textcolor{black}{(P$_{9}$) BL+FCNN$_{all}$+SWE$_{place}$}   &    22.4      &   63.7   &  77.1  & 7   & 19.5  & 73.1 & 5    \\ 

%\hline 

%\textcolor{black}{(P$_{10}$) BL+TDP$_{objects}$+FCNN$_{all}$+SWE$_{object}$}   &   22.4      &   63.7   &  75.5  & 8   & 19.5  & 76.9 & 4     \\ 
%\textcolor{black}{(P$_{10}$) BL+TDP$_{objects}$+FCNN$_{all}$+SWE$_{place}$}   &    22.5      &   63.8   &  75.7  & 8   & 20.2  & 64.9 & 9    \\ 


%\hline
  
 %\textcolor{black}{(P$_{9}$) BL+FCNN$_{all}$+DSWE$_{places+place}$ }  &     22.3  &   63.3     &  81.6   & 5    &  19.5    &  76.9   & 4  \\

%\textcolor{black}{(P$_{9}$) BL+FCNN$_{all}$+DSWE$_{object+object}$ }  &     22.4 &   63.7     &  82.0   & 5    & 19.4    &  76.7   & 4 \\


%\textcolor{black}{(P$_{9}$) BL+FCNN$_{all}$+DSWE$_{object+place}$ }  &   22.4  &   63.7   & 82.0  &  5   &  19.7    &  77.8 & 4  \\ 
  



\hline 
  
%\textcolor{black}{(P$_{10}$) BL+TDP$_{objects}$+FCNN$_{all}$+DSWE$_{places+place}$ }  &     22.5  &   63.3     &  81.6   & 5    &  19.9    &  78.6   & 4  \\

%\textcolor{black}{(P$_{10}$) BL+TDP$_{objects}$FCNN$_{all}$+DSWE$_{object+object}$ }  &     \textbf{22.6}  &  \textbf{64.1}    &  76.1  & 8   & 19.6    &  77.3   & 4 \\


%\textcolor{black}{(P$_{10}$) BL+TDP$_{objects}$+FCNN$_{all}$+DSWE$_{object+place}$ }  &   22.5  &   63.8   & 75.7  &  8   &  19.8    &  78.2   & 4  \\
%\hline 


%\textcolor{black}{(P$_{7}$) BL+FCNN$_{all}$+TWE$_{object}$}   &   \textbf{22.6} &   \textbf{64.1}   &  \textbf{82.6}  &  \textbf{5}  & 19.7  & 71.6 & 6  \\ 
%\textcolor{black}{(P$_{11}$) BL+TDP$_{objects}$+FCNN$_{all}$+TWE$_{object}$}   &   22.5 &   64.0   &  82.4  &  5  & \textbf{20.3}  & \textbf{68.1} & \textbf{9}  \\ 

\hline  

\end{tabular}
}
\footnotetext{Footnote}
\label{table_1} %% original label  
\end{table}





\subsection{Discussion}

The visual context information re-ranks potential candidate words based on the semantic relatedness with its visual information. However, there are some cases when there is no direct semantic correlation between the visual context and the potential word.  Thus we proposed TDP to address this limitation by learning correlations from the training dataset. However, there are still  cases unseen in the training dataset, for instance, as in Figure \ref{fig:SWE} the brand name \textit{zara} and the visual context \textit{crosswalk} or \textit{plaza}. We overcome the limitation of the previous model that learn the semantic between words by adding more information such description what is the images. 

\textcolor{black}{In addition, our approach overcomes some of the limitations that current state-of-the-art deep model struggle to solve in complex background text spotting scenarios, such as short words. For example,  Figure \ref{fig:caption} \textit{kia}, \textit{14} and \textit{wii} all short word have been re-ranked correctly thank to the visual context information.}

\textcolor{black}{Figure~\ref{fig:caption} presents some examples of candidate re-ranking. In the second image,  the word \textit{kia} is ranked up thanks to the words \textit{tennis court} while the detected objects and places are either wrong or have a much more distant relationship. Note that \textit{kia} and \textit{tennis court} have no semantic relation in general, but frequently co-occur in training data, which is captured by TDP. The third image shows a false positive (text \textit{14} and context \textit{group, people} solved just by chance). Finally, last image shows a wrong caption, and a false positive object, but the candidate word was correctly solved thanks to the place classifier.}
 
\textcolor{black}{One limitation of this approach SWE/DSWE that when the text in images is not related to its environmental context, the language model act based on general text, word frequencies, to re-rank the text hypothesis,  which is unpractical for rare words or unseen word in the corpus. Also, when language model false positive is stronger than the visual context re-ranker. For instance,  the word \textit{ohh} has a large frequency count in general text. This problem can be tackled by adjusting the weight of uncommon short words in the language model.} 



\section{Conclusion}

In this paper we have proposed a simple-post processing approach, a hypothesis re-ranker based on visual context information, to improve the accuracy of any pre-trained text spotting system. \textcolor{black}{We  also show that by integrating a hybrid re-ranker, deep learning and statistical language modeling, that based on natural language understanding as a prior to the visual re-ranker, the performance of the visual context re-ranker can be improved. We have shown that the accuracy of two state-of-the-art deep network architectures, a lexicon-based and lexicon-free recognition, can be boosted up to 2.9 percentage-points on standard benchmarks. However, since text in images is not always related to its environment (e.g. a commercial ad of a popular soft drink may appear almost anywhere), there is only a fraction of cases this approach may help solving, but given its low cost, it may be useful for domain adaptation of general text spotting systems.}

In the future work, we plan to explore end-to-end based fusion schemes that can automatically discover more proper priors in one shot deep model fusion architecture. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Chapter 5%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\chapter{Learning to Re-rank with Neural Network (sentence level)}

\section{learning to re-rank}

Our proposed architecture is shown in figure \ref{fig:fcnn}, which consists of 4 main components: 1) word model through Convolution Neural Network (CNN), 2) sentence model through Long Short Memory (LSTM), 3) Attention Mechanism (AT), and 4) Overlap layer 
\subsection{N-gram extraction through Convolution (word model)}
The one-dimensional CNN text encoder for NLP task involved applying a filter window (kernel) over each word in the sentence to extract the N-gram features for different positions. Let $x_{i} \in \mathbb{R}^{d}$ be the word vector for the $i$-th word in the sentence. Let $x \in \mathbb{R}^{s\times d}$ be the input sentence matrix where $s$ is the length of the sentence. let $k$ be the length of each kernel, where $c$ vector is a kernel for each convolution operation $c \in \mathbb{R}^{k\times d}$. For each word position $n$ in the sentence, there is a window vector $w_{n}$ with $k$ word vectors, i.e.: $w_{n} =[x_{n}, x_{n+1},\ldots,x_{n+k-1}]$. The convolution kernel $c$ extracts $k$-gram features at each position in the sentence generating a feature map  $m \in \mathbb{R}^{n-k+1}$, where each element $m_{i}$ of feature map for window vector $w_{n}$ is $m_{j}=f\left(\mathbf{w}_{n} \circ \mathbf{c}+b\right)$ where $f$ is non-linear function \textit{relu} \cite{nair2010rectified} and $b \in \mathbb{R}$ is the bias. 

Our proposed architecture uses multiple kernels to generate the feature maps; thus, it can be arranged as feature representation for each window $w_{n}$ as $W = [m_{1};m_{2};\ldots;m_{n}$] (where semicolons denote column vector concatenation). For each raw $W_{n}$ of $W \in \mathbb{R}^{s-k+1\times f_{n}}$ a new feature is generated from each $f_{n}$ kernel for the window vector at position $n$, where that vector is fed into the LSTM described in the next section. 

\begin{figure}[t!]
 \centering 
% %\includegraphics[width=2.5in]{LM-mod.jpg} 
% \includegraphics[width=3.7in]{0.pdf} % tv examp
%\includegraphics[width=\textwidth]{overview-ICDAR2019.pdf}  
\includegraphics[width=\textwidth]{BMVC-overview.pdf}  
\caption{System Overview. We propose a simple post-process methodology that can be applied to the output of a state-of-the-art DNN. Our system   re-ranks the candidate words using their semantic relatedness with contextual image information  such as objects, scene, and textual captions. In the example, the word \textit{way} has been correctly re-ranked after exploring  the visual information of the  image (which provided labels like \textit{downtown} or \textit{street}).}
 \label{fig:overview} 
 \end{figure} 
%\end{comment}


\subsection{N-gram extraction through Convolution (word model)}
The one-dimensional CNN text encoder for NLP task involved applying a filter window (kernel) over each word in the sentence to extract the N-gram features for different positions. Let $x_{i} \in \mathbb{R}^{d}$ be the word vector for the $i$-th word in the sentence. Let $x \in \mathbb{R}^{s\times d}$ be the input sentence matrix where $s$ is the length of the sentence. let $k$ be the length of each kernel, where $c$ vector is a kernel for each convolution operation $c \in \mathbb{R}^{k\times d}$. For each word position $n$ in the sentence, there is a window vector $w_{n}$ with $k$ word vectors, i.e.: $w_{n} =[x_{n}, x_{n+1},\ldots,x_{n+k-1}]$. The convolution kernel $c$ extracts $k$-gram features at each position in the sentence generating a feature map  $m \in \mathbb{R}^{n-k+1}$, where each element $m_{i}$ of feature map for window vector $w_{n}$ is $m_{j}=f\left(\mathbf{w}_{n} \circ \mathbf{c}+b\right)$ where $f$ is non-linear function \textit{relu} \cite{nair2010rectified} and $b \in \mathbb{R}$ is the bias. 

Our proposed architecture uses multiple kernels to generate the feature maps; thus, it can be arranged as feature representation for each window $w_{n}$ as $W = [m_{1};m_{2};\ldots;m_{n}$] (where semicolons denote column vector concatenation). For each raw $W_{n}$ of $W \in \mathbb{R}^{s-k+1\times f_{n}}$ a new feature is generated from each $f_{n}$ kernel for the window vector at position $n$, where that vector is fed into the LSTM described in the next section. 


Following \cite{zhou2015c}, we discard using pooling layer, maxpooling or dynamic $k$-max pooling, which are often applied after the convolution feature map. The pooling breaks the sequence that we are interested to learn, by stacking an LSTM on the top of the CNN.


%\subsection{Learning the higher level feature with LSTM (sentence model)} 
\subsection{Learning the higher level feature with LSTM} 
Long Short-Term Memory (LSTM) \cite{hochreiter1997long} is able to capture historical information from sequential input sequences. The architecture can handle sequential information as the current input $x_t$ can access the previous output, hidden layer $h_{t-1}$, at each time step.

The advantages of LSTM over standard Recurrent Neural Networks (RNN) relies on $R$ \textbf{gates} that control the output of each time step, as a function of the previous/old hidden state $h_{t-1}$ and the current time step input $x_t$: 1) forget gate $f_t$, 2) input gate $i_t$, and finally 3) output gate $f_t$. These gates can control (update, reject) the memory cell $c_t$ and the hidden state $h_t$. The LSTM transition function is defined as :

 
 \begin {equation} 
\begin{aligned} i_{t} &=\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right), \;
%\begin{aligned} i_{t} &=\sigma\left(W_{n} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right), \;
f_{t} =\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right) \\
q_{t} &=\tanh \left(W_{q} \cdot\left[h_{t-1}, x_{t}\right]+b_{q}\right), \;
o_{t} =\sigma\left(W_{o} \cdot\left[h_{t-1}, x_{t}\right]+b_{o}\right) \\
c_{t} &=f_{t} \odot c_{t-1}+i_{t} \odot q_{t},  \;
h_{t} =o_{t} \odot \tanh \left(c_{t}\right)
\end{aligned}
 \end {equation}

Where $\sigma$ is a logistic sigmoid function [0,1] and \textit{tanh} as hyperbolic tangent function [-1,1] and  $\odot$  denotes as element-wise multiplication. $f_t$ is the function that controls the information from the old memory cell $c_t$ (update, reject), $i_t$ is the function to control how much information to be stored in the current memory cell $c_t$ and $o_t$ is to control what is the output based from $c_t$. 

We stack an LSTM after the convolution layer to learn the higher-level feature of the long dependencies in the sequence. 



\begin{figure}[t!]
 \centering 
\includegraphics[width=\textwidth]{AT-BMVC.pdf}
%\includegraphics[width=6.9in]{ICDAR2019-visual_v1.pdf} 
 \caption{Overview of the system pipeline, an end-to-end fashion post-process re-ranks potential words based on their semantic relatedness with the context in the image (objects, scenarios, natural language descriptions, ...). \textit{(best viewed in color) }} %\francescrmk{\textcolor{pink}{This image should be shown before in the paper,} and would be better in double column}} %In the example, the word \textit{quarters} has been re-ranked overcoming other candidates thanks to the detected objects/places (c$_i$) \textit{parking, street, meter}, as shown in the table above.}  %See table \ref{table_1}.}
 %Note that the blue box corresponds to the `Language Model Re-Ranker' box in Fig.~\ref{lm-model1}.} %\francescrmk{'Text spotting pipeline' should say: ' Text Spotting Baseline'. The outer box of the Language Model Re-ranker should be in blue to match the color in Fig. 1.} } 
\label{fig:fcnn}
% \label{table_1}
\end{figure}








\subsection{Attention Mechanism}
\label{sec:Attention}
%Attention mechanism architecture show remrablere result in macheinle translation  \cite{bahdanau2014neural}

Attention-based models have shown a promising result on various NLP tasks, such as machine translation \cite{bahdanau2014neural}. Such mechanism learns to focus the attention on a specific part of the input (e.g. target word in a sentence).   

We apply an attention mechanism \cite{raffel2015feed} via LSTM to capture the temporal dependencies in the sequence. This attention uses Feed Forward Neural Network (FFN) attention function:
\begin{equation}
e_{t}=\tanh \left(h_{t} W_{a}\right) v_{a}^{\mathrm{T}} 
\end{equation}
\noindent where $W_{a}$ is the attention of the hidden weight matrix and $v_{a}$ is the output vector.  As shown in Figure \ref{fig:fcnn} the vector $c$ is computed as a weighted average of $h_{t}$, given by $\alpha$. Thus, the attention mechanism is used to produce a single vector $c$ from the complete sequence as follows: 

%For example the model see parking-street and street-parking  are the same.   
\begin{equation}
e_{t}=a\left(h_{t}\right), 
\alpha_{t}=\frac{\exp \left(e_{t}\right)}{\sum_{k=1}^{T} \exp \left(e_{k}\right)}, c=\sum_{t=1}^{T} \alpha_{t} h_{t} 
\end{equation}

%where $\alpha$ is learnable function depened on $h_{t}$ 
where $T$ is the total number of steps and $\alpha_{t}$ is the computed weight of each time step $t$ for each state $h_{t}$, $a$ is a learnable function depends only on $h_{t}$, unlike standard attention mechanism $(s_{t-1}, h_{t})$ as it computes a scalar value of $h_{t}$ given the value of $h_{t}$ at previous state sequence $s_{t-1}$. As this attention is computing the average over time, its discards the temporal order, which is ideal for learning semantic relations between words. In short, the attention gives higher weights to more important words in the sentence without relying on sequence order. 

\subsection{Overlap layer}
The main idea of the overlap layer is to emphasize the most visual information happen in that image by counting the overlap visual information (object, place, and caption). As shown in Figure \ref{fig:fcnn}  \textit{tennis} happen 3 times and ball \textit{2} times, that indicate that the most import visual in that image is  \textit{tennis match}   


\section{Proposed Model} 

We propose a multi-channel Convolutional-LSTM with attention mechanism that is able to learn the semantic relatedness between the visual context information and the candidate word, in an end-to-end fashion. The network consists of three inputs (candidate word, caption, and external visual information). 
%The joint layers then concatenated and fed to a loss function which calculates the semantic relatedness between both inputs. This architecture is mostly used in  QA tasks.}
%This architecture is mostly used in tasks oriented to state equality/difference between two inputs in QA, or to produce a degree of similarity between them.}% cite 

Our architecture is inspired by \cite{severyn2015learning}, that proposed CNN based re-rankers to re-rank possible answers to a given question (QA). However, our network consists of two subnetworks each of them with 4-channels with kernel sizes $k=(3,3,5,8)$, and overlap layer as following: 

%1) multi-channel convolution  (MCC) 2) convolution-LSTM  with (FNN) attention (CLSTM-A).  }

\noindent{\bf{Multi-Channel Convolution}}: The first subnetwork is, only convolution kernels, to extract n-gram or keywords features from the sequence (sentence). 

\noindent{\bf{Multi-Channel Convolution-LSTM}}: following C-LSTM \cite{zhou2015c} the second convolution kernels are fed into an LSTM. The LSTM captures the long term dependencies over the feature sequence from the previous layers CNN. However, we add an attention mechanism to capture the most important features from that sequence. The advantage of adding attention is that $c$ is computed as the weight of an average of $h_{t}$ given $\alpha$ over time, thus the model learns the sequence without relying on the temporal order.

In both CNNs we discard the pooling layer because down-sampling separate the selected feature in case of first CNN and breaks the sequence before the LSTM.


%In both CNNs we discard a pooling layer because down-sampling discontinuous the selected feature in case of first CNN and breaks the sequence before the LSTM.%\francesc{This sentence is not clear. "discontinuous" is not a verb}

\noindent{\bf{Overlap Layer}:} the overlap layer is just a frequency count dictionary to compute any overlap information from the inputs. The idea is to place more weight to the most frequent visual element, specially when it is observed by more than one visual classifier. The dictionary output is a fully connected layer.  

Finally, we merge all subnetworks into a joint layer that is fed to a loss function which calculates the semantic relatedness between both inputs. We call the combined model Fusion Dual Convolution-LSTM (FDCLSTM$_{AT}$). Also, we introduce a simpler model a Convolution-LSTM without attention (FCLSTM).     
\medskip
 
%\textcolor{blue}{The bottom two CNNs processes the spotted candidate word, and the second (top) CNNs handles the visual information. The model is a multi-channels based architecture with 4-channels ($k$ = 3, 3, 5, 8) kernel size. This first convulation kernels extract n-gram from the sequence. The second CLSTM-A extract most important feature from the sequence. In both CNNs we discard pooling layer because down-sampling break the sequence . }  

Since we have only one spotted word per image, we use a convolution with \textit{binary masking} in the candidate word side (first channel). Thus, the candidate word passes into the joint layer. We concatenate CNN outputs with the additional feature into MLPs layers, and finally a sigmoid layer performing binary classification. We trained the model with a binary cross-entropy loss ($l$) where the target value (in $[0,1]$) is the semantic relatedness between the word and the visual. Instead of restricting ourselves to a simple similarity function, we let the network learn the margin between the two classes --i.e. the degree of similarity. For this, we increase the depth of network after the MLPs merge layer with more fully connected layers. The network is trained through Nesterov-accelerated Adam (Nadam) \cite{dozat2016incorporating} as the author show that Nesterov momentum consistently achieve better results, in cases such as word vectors/neural language modelling, than other optimizers using only classical momentum (ADAM). Also, we apply 50\% dropout between each convolution layer with batch normalization \cite{ioffe2015batch}, and 70\% between each MLPs for regularization purposes.





%\section{Attention}
%\subsection{Attention-1}
%\subsection{Attention-2}
%\subsection{Attention-3}
%\section{our}
%\subsection{FDCLSTM-A}
%\subsubsection{FCNN}
%\subsubsection{FCLSTM}
%\subsubsection{FCLSTM with attention}


%\subsection{Attentive CNN} 
%\subsubsection{Attentive LSTM}
%\subsubsection {Attentive with Mask CNN}

%\section{Sentence embedding}
%\subsection{Bert}
%\subsection{Universal sentence Encoder}
%\subsection{Infrasg}

%\section{Overlap layers}
%The main idea of the overlap layer is to emphasize the most visual information happen in that image by counting the overlap visual information (object, place, and caption). As shown in Figure \ref{fig:fcnn}  \textit{tennis} happen 3 times and ball \textit{2} times, that indicate that the most import visual in that image is  \textit{tennis match}    

\section{Experiments}
The reported experiments consists of applying our different re-rankers to reorder the $k$-best hypothesis produced by an off-the-shelf state-of-the-art text spotting system. We experimented extracting $k$-best hypothesis for $k=3..10$.

We use two pre-trained deep models: a CNN \cite{jaderberg2016reading} and an LSTM \cite{ghosh2017visual} as baselines (BL) to extract the initial list of word hypotheses.

Since these BLs need to be fed with cropped words (i.e. image regions containing only text), when evaluating on the ICDAR-2017-Task3 dataset we will use the ground truth bounding boxes of the words. Thus, we are not evaluating a whole end-to-end system (text box detection and recognition), but only the influence of adding external knowledge to the second stage (recognition).

The CNN baseline uses a closed lexicon; therefore, it cannot recognize any word outside its dictionary.  Table~\ref{table_results} presents three different accuracy metrics for this case: 1) \textbf{full} columns correspond to the accuracy on the whole dataset. 2) \textbf{dict} columns correspond to the accuracy over the cases where the target word is among the 90K-words of the CNN dictionary (which correspond to 43.3\% of the whole dataset 3) \textbf{list} columns show the accuracy over the cases where the right word was among the $k$-best produced by the baseline. 


The LSTM baseline does not rely on a closed lexicon, so only \textit{full} and \textit{list} columns are provided.

Results in Table~\ref{table_results} are the highest result among all proposed neural Re-ranker, and were obtained using Inception-ResNet-v2 \cite{szegedy2017inception} as object classifier, \textit{Place365-ResNet152} \cite{zhou2017places} as scene detection, and \textit{Neural Image Caption Generator} (NIC) for caption generation \cite{vinyals2015show}.





%The CNN baseline uses a closed lexicon, so it can not recognize any word outside its dictionary. Table~\ref{table_1} presents three different accuracy metrics for this case:
%\begin{enumerate}
%\itemsep 0cm
%    \item \textit{full} columns correspond to the accuracy on the whole dataset. \item \textit{dict} columns correspond to the accuracy over the cases where the target word is among the 90K-words of the CNN dictionary (which correspond to 43.3\% of the whole dataset)
%    \item \textit{list} columns show the accuracy over the cases where the right word was in the $k$-best list output by the baseline. 
%\end{enumerate}
    
%The LSTM baseline does not rely on a closed lexicon, so only \textit{full} and \textit{list} columns are provided.

%Results in Table~\ref{table_1} are the highest among all tested combinations, and were obtained using Inception-ResNet-v2 \cite{szegedy2017inception} as object classifier, \textit{Place365-ResNet152} \cite{zhou2017places} as scene detection, and \textit{Neural Image Caption Generator} (NIC) for caption generation \cite{vinyals2015show}.

In this experiment we re-rank the baseline $k$-best hypotheses based on their relatedness with the image objects, scenario, and/or caption. 
we trained a convolutional-LSTM neural network with an attention mechanism to learn the semantic relatedness between a caption and a candidate word. We train the model from scratch on the training data, initializing the weights with the pre-trained non-static dual channels general embeddings.   

%We apply a binary masking with convolution layer that extracts the candidate word. Thus, the text hypothesis passes without any padding to the final joint layer as in Figure \ref{fig:fcnn}. In this case, padding the sequence with zero has negative impact on the learning stability of the network. In addition, the minimum size that the kernel can extract important information from the sequence in this architectures is tri-gram \cite{zhang2015sensitivity}. Thus, we apply two tri-grams kernels, one with masking (unigram), and the second one is to extract most important key word from the sequence. The rest of the kernels are 5-grams and 8 grams for extracting different information from the caption. As shown in Figure \ref{fig:fcnn}, the extracted word feature from the convolutional kernals are fed into two subnetworks and overlap layer: 1) the extracted feature as keywords/n-gram are fed into the joint layers 2) we stack an LSTM with FFN attention model. The LSTM process the sentence (caption), and the attention assigned more weight to the import word in that sequence. In addition, the attention mechanism is not sensitive to temporal order of the sequence. Finally, the overlap layer is a dictionary that computes the overlap between all information across the network (text hypothesis, caption, object, scene).  \francesc{All this paragaph is repeating things that were explained before. No need.}


\begin{figure*}[t!]
\centering 
% %\includegraphics[width=2.5in]{LM-mod.jpg} 
% \includegraphics[width=3.7in]{0.pdf} % tv examp
\includegraphics[width=\textwidth]{overview-ICDAR2019-visual.pdf}   %without the left bar
%\includegraphics[width=6.9in]{ICCV_caption.pdf}   %without the left bar
%\includegraphics[width=4.7in]{caption_example_v3.pdf}
\caption{An example illustrate that natural language description overcome the limitation of  visual context, such as object and scene, and provide more visual context information such as action, location and specific details.} %(The variation of color shows the degree of semantic relatedness score with the candidate word).}
 %\caption{Overview of the system pipeline. A post-process re-ranks potential words based on their semantic relatedness with the context in the image (objects, scenarios, ...). In the example, the word \textit{quarters} has been re-ranked overcoming other candidates thanks to the detected objects/places (c$_i$) \textit{parking, street, meter}, as shown in the table above.}  %See table \ref{table_1}.}
 %Note that the blue box corresponds to the `Language Model Re-Ranker' box in Fig.~\ref{lm-model1}.} %\francescrmk{'Text spotting pipeline' should say: ' Text Spotting Baseline'. The outer box of the Language Model Re-ranker should be in blue to match the color in Fig. 1.} } 
 \label{fig:caption}
% \label{table_1}
 \end{figure*} 





We apply a binary masking with convolution layer that extracts the candidate word. Thus, the text hypothesis passes without any padding to the final joint layer as in Figure \ref{fig:fcnn}. In this case, padding the sequence with zero has negative impact on the learning stability of the network. As shown in Figure \ref{fig:fcnn}, the extracted word feature from the convolutional kernels are fed into two subnetworks and overlap layer.


%\noindent{\bf TWE:} \citep{Ahmed:18} Semantic Relatedness with Word Embedding (TWE) \cite{Tomas:13}. A word embedding that used to lean the relation between text and visual context. Our work outperforms  \citep{Ahmed:18}, who trained a word embedding \citep{Tomas:13} from scratch on the same task. This work uses pre-possessing technique to force the visual context to the candidate word. Also, the author proposed a method to convert the similarity to a probability that based on an approved assumption \citep{Sergey:03}.
We compare our proposed model with different approaches able to capture/extract semantic information such as keywords from the text. Thus, the chosen approaches are not sensitive to word order in the sequence. We follow the same setup proposed by \cite{severyn2015learning} for QA ranking problem.

\noindent{\bf Attentive LSTM:} \citep{Ming:16}  QA-LSTM based siamese network that uses word-level attention to gives more weight to certain words in the other input sentence. Thus the computed weight is taking into consideration the information from the other input sentence in word level manner.  Since our goal is the semantic relatedness score, we discard the similarity function proposed by the author. This architecture is very similar to ours; however, our proposed model relies more on n-grams level as CNN is used as feature keyword extraction.       

\noindent{\bf MVCNN:} \citep{Wenpeng:16} Multichannel variable-size convolution network that extracts multiple phrases with variable size kernel. MVCNN combines multiple versions of pre-traind embedding to extract extra information from the same set of word vector. This model achieves good result in small scale data (binary task). 

%\noindent{\bf DCNN:} \citep{Nal:14} Dynamic Convolutional Neural Network using a feature graph able to capture textual relations among variable length texts. DCNN uses dynamic k-pooling that, different from local max-pooling, output k-max value of different convolutional dimension (sentences with different length) of the previous convolutional layer. This architecture  achieves high performance in question answering and semantic modeling of text. 

\noindent{\bf CNN+RNN}: \citep{Xingyou:16}  CNN-RNN is a Multichannel CNN with RNN coupling. The RNN is added, over the CNN, to learn the local features generate by CNN and learn the long-distance dependencies in short text. In our case the RNN (LSTM) \citep{Sepp:97} is trained to process the caption as sequence, as the CNN  is used to extract keywords feature like n-grams and thus; make the sequence short (downsample). We aggregate this information with last joint layer as extra information from the caption alone.     

\noindent{\bf MG(NC)-CNN):} \citep{Ye:16} Multi-group norm constraint convolutional network that captures multiple information from multiple sets of embedding. Constraint regularization is applied to capture diffident properties of text by penalizing weight that less discriminative.  \\ 

\noindent{\bf C-LSTM:} \citep{Chunting:15} Multichannel convolution layers, to capture the high-level feature, that fed into an LSTM to learn the global and temporal relation from the sequence. Unlike CNN+RNN that uses pooling layer to downsample the sequence, C-LSTM discards the pooling layer. The LSTM lean the extracted sequence from the previous layer CNN directly without any feature downsampling. Our work is inspired by this architecture.  


\noindent{\bf USE-Transformer:} \citep{Daniel:18} Universal Sentence Encoder (USE) current State-of-the-art in Semantic Textual Similarity (STS). This work proposed two USE model that uses different encoder architectures to achieve distinct design goals. The first one is based on the transformer architecture USE-T \citep{Ashish:17} that target high accuracy as cost of complexity and resource consumption. The second one is based on Deep Averaging Network that targets efficient inference with slightly reduced accuracy. We experimental with USE-T fine tuning and extracting feature to compute the semantic relation with cosine distance. We achieved the best result with word-to-sentence; however, we extended our experiment to word-to-word to be able to compare with different word-to-word methods as shown in the Table \ref{result}. As the USE-T$_{sentence}$ the USE-T$_{word}$ is require a shorter hypothesis list, less $k$, to get the top performance when the right word is in the hypothesis list (better retrieval score than the other methods). 

\noindent{\bf Bert:} \citep{Jacob:18} (Bidirectional Encoder Representations from Transformers) has shown groundbreaking result in many task such as Question Answer and Natural Language Inference. However, according to the main authors of the Bert is not suited for this task, Semantic Textual Similarity (STS), thus; it does not generate a meaningful vector to compute the cosine distance. In this experiment we fine tune the model with one layer with the cosine function to compute semantic score between caption and candidate word. 


\begin{figure*}[t!]
\centering 
\includegraphics[width=\textwidth]{all-example-EMNLP.pdf} 
\caption{An example illustrate some examples of candidate re-ranking. The top three examples are re-ranked  based on the semantic relatedness score. The \textit{delta}-\textit{airliner} which frequently co-occur in training data is captured by overlap layers. The \textit{12}-\textit{football} show that the relation between sport and numbers. Also, \textit{program}-\textit{school} have a much more distance relation but our model is able to re-rank the most related words. The \textit{blue-runway} and \textit{bus-private} show that the overlap layer can be effective when the visual context appears in more than one visual context classifier. Finally \textit{hotdog}-\textit{a} and \textit{by}-\textit{ski} have no semantic correlation but are solved by network thanks to the frequency count dictionary. }
%Language Model
%thanks to the frequency count dictionary
 \label{fig:caption}
% \label{table_1}
 \end{figure*} 








%\subsection{datasets}
%\subsection{Implementation Details}
\subsection{Result}
\section{Conclusion}


In this paper we have proposed a simple-post processing approach for text spotting. We use state-of-the-art out-of-the-box DNNs as baselines, and we re-rank the $k$-best hypotheses they produce using their relatedness to other elements in the image context (objects, places, caption), which are also computed using out-of-the-box systems. Results show that this introduction of contextual semantic information significatively increases performance of the baseline. Since text in images is not always related to its environment (e.g. a commercial ad of a soft drink may appear almost anywhere), there is only a fraction of cases this approach may help solving, but given its low cost, it may be useful for domain adaptation of general text spotting systems. 


%\section 

\chapter{Combining Re-ranker}
\section{}




%\chapter{COCO-text experiments}
\begin{equation}
k_1=\frac{\omega }{c({1/\varepsilon_m + 1/\varepsilon_i})^{1/2}}=k_2=\frac{\omega
sin(\theta)\varepsilon_{air}^{1/2}}{c}
\end{equation}


\chapter{Summary and Future Work}
\section{achievements}
\section{Future Work}
\noindent
where $\omega$ is the frequency of the plasmon, $c$ is the speed of
light, $\varepsilon_m$ is the dielectric constant of the metal,
$\varepsilon_i$ is the dielectric constant of neighboring insulator,
and $\varepsilon_{air}$ is the dielectric constant of air.



\chapter{Chapter 3}

\chapter{Chapter 4}

\appendix
\chapter{The Design of Mask Convolution Architectures}
\chapter{Survey and human evaluation}
Appendix chapter 1 text goes here

%\bibliography{references}

%\section*{References}
%\bibliographystyle{hangcaption}

\bibliography{sampleThesis}

%\bibliography{references}
\end{document}